{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Lift your model performance with Learning Rate Schedules !!\n",
    "Training a large deep learning model is a diﬃcult optimization task. The classical algorithm to train neural networks is called **stochastic gradient descent**. It has been well established that you can achieve increased performance and faster training on some problems by using a **learning rate** that changes during training.\n",
    "- a time-based learning rate schedule\n",
    "- a drop-based learning rate schedule\n",
    "\n",
    "Adapting the **learning rate** for your **stochastic gradient descent** optimization procedure can increase performance and reduce training time. Sometimes this is called **learning rate annealing** or **adaptive learning rates**. Here we will call this approach a learning rate schedule, where the default schedule is to use a **constant learning rate** to update network weights for each training epoch. \n",
    "\n",
    "The simplest and perhaps most used adaptation of learning rates during training are techniques that **reduce the learning rate over time**. These have the beneﬁt of making large changes at the beginning of the training procedure when larger learning rate values are used, and decreasing the learning rate such that a smaller rate and therefore smaller training updates are made to weights later in the training procedure. **This has the eﬀect of quickly learning good weights early and ﬁne tuning them later**. Two popular and easy to use learning rate schedules are as follows:\n",
    "- Decrease the learning rate gradually based on the **epoch**\n",
    "- Decrease the learning rate using punctuated **large drops** at speciﬁc epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dataset\n",
    "The **Ionosphere binary classiﬁcation problem** is used as a demonstration in this lesson. The dataset describes **radar returns** where the target was free electrons in the ionosphere. It is a binary classiﬁcation problem where positive cases (g for good) show **evidence of some type of structure** in the ionosphere and negative cases (b for bad) do not. It is a good dataset for practicing with neural networks because all of the inputs are small numerical values of the same scale. \n",
    "\n",
    "There are **34 attributes and 351 observations**. State-of-the-art results on this dataset achieve an accuracy of approximately 94% to 98% accuracy using 10-fold cross-validation. You can learn more about the ionosphere dataset on the UCI Machine Learning Repository website."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "      <th>32</th>\n",
       "      <th>33</th>\n",
       "      <th>34</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.99539</td>\n",
       "      <td>-0.05889</td>\n",
       "      <td>0.85243</td>\n",
       "      <td>0.02306</td>\n",
       "      <td>0.83398</td>\n",
       "      <td>-0.37708</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.03760</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.51171</td>\n",
       "      <td>0.41078</td>\n",
       "      <td>-0.46168</td>\n",
       "      <td>0.21266</td>\n",
       "      <td>-0.34090</td>\n",
       "      <td>0.42267</td>\n",
       "      <td>-0.54487</td>\n",
       "      <td>0.18641</td>\n",
       "      <td>-0.45300</td>\n",
       "      <td>g</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>-0.18829</td>\n",
       "      <td>0.93035</td>\n",
       "      <td>-0.36156</td>\n",
       "      <td>-0.10868</td>\n",
       "      <td>-0.93597</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.04549</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.26569</td>\n",
       "      <td>-0.20468</td>\n",
       "      <td>-0.18401</td>\n",
       "      <td>-0.19040</td>\n",
       "      <td>-0.11593</td>\n",
       "      <td>-0.16626</td>\n",
       "      <td>-0.06288</td>\n",
       "      <td>-0.13738</td>\n",
       "      <td>-0.02447</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 35 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   0   1        2        3        4        5        6        7    8        9   \\\n",
       "0   1   0  0.99539 -0.05889  0.85243  0.02306  0.83398 -0.37708  1.0  0.03760   \n",
       "1   1   0  1.00000 -0.18829  0.93035 -0.36156 -0.10868 -0.93597  1.0 -0.04549   \n",
       "\n",
       "  ...       25       26       27       28       29       30       31       32  \\\n",
       "0 ... -0.51171  0.41078 -0.46168  0.21266 -0.34090  0.42267 -0.54487  0.18641   \n",
       "1 ... -0.26569 -0.20468 -0.18401 -0.19040 -0.11593 -0.16626 -0.06288 -0.13738   \n",
       "\n",
       "        33  34  \n",
       "0 -0.45300   g  \n",
       "1 -0.02447   b  \n",
       "\n",
       "[2 rows x 35 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from keras.optimizers import SGD # Stochastic Gradient Descent\n",
    "\n",
    "np.random.seed(47)\n",
    "df = pd.read_csv('ionosphere.csv', header=None)\n",
    "data = df.values\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = data[:, 0:34].astype(float)\n",
    "\n",
    "y = data[:, -1]\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(y)\n",
    "encoded_y = encoder.transform(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time-Based Learning Rate Schedule\n",
    "The stochastic gradient descent optimization algorithm implementation in the **SGD** class has an argument called **decay**. This argument is used in the **time-based learning rate decay schedule** equation as follows: \n",
    "\n",
    "$LearningRate = LearningRate \\times \\frac{1}{1 + (decay \\times epoch)}$\n",
    "\n",
    "When the **decay** argument becomes zero (the default), this has no eﬀect on the learning rate..\n",
    "- LearningRate = $0.1 \\times \\frac{1}{1 + (0.0 \\times 1)}$ \n",
    "- LearningRate = $0.1$\n",
    "\n",
    "When the decay argument is speciﬁed, it will decrease the learning rate from the previous epoch by the given ﬁxed amount. For example, if we use the initial learning rate value of 0.1 and the decay of 0.001...\n",
    "<img src=\"46.jpg\">\n",
    "\n",
    "You can create a nice default schedule by **setting the decay value** as follows:\n",
    "- Decay = LearningRate / Epochs \n",
    "- Decay = 0.1 / 100 \n",
    "- Decay = 0.001\n",
    "\n",
    "The example below demonstrates using the time-based learning rate adaptation schedule in Keras. A small neural network model is constructed with a single hidden layer with **34 neurons** and using the **rectiﬁer activation** function. The output layer has a single neuron and uses the sigmoid activation function in order to output probability-like values. The learning rate for stochastic gradient descent has been set to a higher value of **0.1**. The model is trained for 50 epochs and the decay argument has been set to 0.002. Additionally, it can be a good idea to use **momentum** when using an adaptive learning rate. In this case we use a **momentum value of 0.8**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create the model\n",
    "model = Sequential() \n",
    "model.add(Dense(34, input_dim=34, kernel_initializer='normal', activation='relu')) \n",
    "model.add(Dense(1, kernel_initializer='normal', activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 235 samples, validate on 116 samples\n",
      "Epoch 1/50\n",
      " - 0s - loss: 0.0174 - acc: 0.9957 - val_loss: 0.0725 - val_acc: 0.9741\n",
      "Epoch 2/50\n",
      " - 0s - loss: 0.0172 - acc: 0.9957 - val_loss: 0.0725 - val_acc: 0.9741\n",
      "Epoch 3/50\n",
      " - 0s - loss: 0.0186 - acc: 0.9957 - val_loss: 0.0826 - val_acc: 0.9741\n",
      "Epoch 4/50\n",
      " - 0s - loss: 0.0182 - acc: 0.9957 - val_loss: 0.0745 - val_acc: 0.9741\n",
      "Epoch 5/50\n",
      " - 0s - loss: 0.0180 - acc: 0.9957 - val_loss: 0.0722 - val_acc: 0.9828\n",
      "Epoch 6/50\n",
      " - 0s - loss: 0.0232 - acc: 0.9957 - val_loss: 0.0820 - val_acc: 0.9741\n",
      "Epoch 7/50\n",
      " - 0s - loss: 0.0169 - acc: 0.9957 - val_loss: 0.0756 - val_acc: 0.9741\n",
      "Epoch 8/50\n",
      " - 0s - loss: 0.0212 - acc: 0.9957 - val_loss: 0.0753 - val_acc: 0.9655\n",
      "Epoch 9/50\n",
      " - 0s - loss: 0.0147 - acc: 0.9957 - val_loss: 0.0903 - val_acc: 0.9655\n",
      "Epoch 10/50\n",
      " - 0s - loss: 0.0170 - acc: 0.9957 - val_loss: 0.0899 - val_acc: 0.9655\n",
      "Epoch 11/50\n",
      " - 0s - loss: 0.0155 - acc: 0.9957 - val_loss: 0.0798 - val_acc: 0.9741\n",
      "Epoch 12/50\n",
      " - 0s - loss: 0.0162 - acc: 0.9957 - val_loss: 0.0807 - val_acc: 0.9741\n",
      "Epoch 13/50\n",
      " - 0s - loss: 0.0172 - acc: 0.9957 - val_loss: 0.0839 - val_acc: 0.9741\n",
      "Epoch 14/50\n",
      " - 0s - loss: 0.0152 - acc: 0.9957 - val_loss: 0.0849 - val_acc: 0.9741\n",
      "Epoch 15/50\n",
      " - 0s - loss: 0.0160 - acc: 0.9957 - val_loss: 0.0879 - val_acc: 0.9741\n",
      "Epoch 16/50\n",
      " - 0s - loss: 0.0207 - acc: 0.9957 - val_loss: 0.0776 - val_acc: 0.9741\n",
      "Epoch 17/50\n",
      " - 0s - loss: 0.0164 - acc: 0.9957 - val_loss: 0.0823 - val_acc: 0.9741\n",
      "Epoch 18/50\n",
      " - 0s - loss: 0.0154 - acc: 0.9957 - val_loss: 0.0840 - val_acc: 0.9741\n",
      "Epoch 19/50\n",
      " - 0s - loss: 0.0146 - acc: 0.9957 - val_loss: 0.0757 - val_acc: 0.9741\n",
      "Epoch 20/50\n",
      " - 0s - loss: 0.0164 - acc: 0.9957 - val_loss: 0.0785 - val_acc: 0.9741\n",
      "Epoch 21/50\n",
      " - 0s - loss: 0.0151 - acc: 0.9957 - val_loss: 0.0810 - val_acc: 0.9741\n",
      "Epoch 22/50\n",
      " - 0s - loss: 0.0148 - acc: 0.9957 - val_loss: 0.0820 - val_acc: 0.9741\n",
      "Epoch 23/50\n",
      " - 0s - loss: 0.0158 - acc: 0.9957 - val_loss: 0.0863 - val_acc: 0.9741\n",
      "Epoch 24/50\n",
      " - 0s - loss: 0.0149 - acc: 0.9957 - val_loss: 0.0793 - val_acc: 0.9741\n",
      "Epoch 25/50\n",
      " - 0s - loss: 0.0142 - acc: 0.9957 - val_loss: 0.0792 - val_acc: 0.9741\n",
      "Epoch 26/50\n",
      " - 0s - loss: 0.0148 - acc: 0.9957 - val_loss: 0.0809 - val_acc: 0.9741\n",
      "Epoch 27/50\n",
      " - 0s - loss: 0.0142 - acc: 0.9957 - val_loss: 0.0909 - val_acc: 0.9741\n",
      "Epoch 28/50\n",
      " - 0s - loss: 0.0145 - acc: 0.9957 - val_loss: 0.0846 - val_acc: 0.9741\n",
      "Epoch 29/50\n",
      " - 0s - loss: 0.0143 - acc: 0.9957 - val_loss: 0.0813 - val_acc: 0.9741\n",
      "Epoch 30/50\n",
      " - 0s - loss: 0.0161 - acc: 0.9957 - val_loss: 0.0836 - val_acc: 0.9741\n",
      "Epoch 31/50\n",
      " - 0s - loss: 0.0138 - acc: 0.9957 - val_loss: 0.0875 - val_acc: 0.9741\n",
      "Epoch 32/50\n",
      " - 0s - loss: 0.0132 - acc: 0.9957 - val_loss: 0.0811 - val_acc: 0.9741\n",
      "Epoch 33/50\n",
      " - 0s - loss: 0.0155 - acc: 0.9957 - val_loss: 0.0816 - val_acc: 0.9741\n",
      "Epoch 34/50\n",
      " - 0s - loss: 0.0128 - acc: 0.9957 - val_loss: 0.0974 - val_acc: 0.9655\n",
      "Epoch 35/50\n",
      " - 0s - loss: 0.0141 - acc: 0.9957 - val_loss: 0.0748 - val_acc: 0.9741\n",
      "Epoch 36/50\n",
      " - 0s - loss: 0.0158 - acc: 0.9957 - val_loss: 0.0847 - val_acc: 0.9655\n",
      "Epoch 37/50\n",
      " - 0s - loss: 0.0139 - acc: 0.9957 - val_loss: 0.0854 - val_acc: 0.9655\n",
      "Epoch 38/50\n",
      " - 0s - loss: 0.0133 - acc: 0.9957 - val_loss: 0.0937 - val_acc: 0.9655\n",
      "Epoch 39/50\n",
      " - 0s - loss: 0.0128 - acc: 0.9957 - val_loss: 0.0801 - val_acc: 0.9741\n",
      "Epoch 40/50\n",
      " - 0s - loss: 0.0170 - acc: 0.9957 - val_loss: 0.0759 - val_acc: 0.9741\n",
      "Epoch 41/50\n",
      " - 0s - loss: 0.0119 - acc: 0.9957 - val_loss: 0.0990 - val_acc: 0.9655\n",
      "Epoch 42/50\n",
      " - 0s - loss: 0.0134 - acc: 0.9957 - val_loss: 0.0844 - val_acc: 0.9741\n",
      "Epoch 43/50\n",
      " - 0s - loss: 0.0131 - acc: 0.9957 - val_loss: 0.0861 - val_acc: 0.9655\n",
      "Epoch 44/50\n",
      " - 0s - loss: 0.0138 - acc: 0.9957 - val_loss: 0.0843 - val_acc: 0.9655\n",
      "Epoch 45/50\n",
      " - 0s - loss: 0.0146 - acc: 0.9957 - val_loss: 0.0796 - val_acc: 0.9655\n",
      "Epoch 46/50\n",
      " - 0s - loss: 0.0126 - acc: 0.9957 - val_loss: 0.0949 - val_acc: 0.9655\n",
      "Epoch 47/50\n",
      " - 0s - loss: 0.0150 - acc: 0.9957 - val_loss: 0.0874 - val_acc: 0.9655\n",
      "Epoch 48/50\n",
      " - 0s - loss: 0.0144 - acc: 1.0000 - val_loss: 0.1054 - val_acc: 0.9655\n",
      "Epoch 49/50\n",
      " - 0s - loss: 0.0152 - acc: 0.9957 - val_loss: 0.0821 - val_acc: 0.9655\n",
      "Epoch 50/50\n",
      " - 0s - loss: 0.0139 - acc: 0.9957 - val_loss: 0.0893 - val_acc: 0.9655\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x224138370f0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compile and fit the model\n",
    "epochs = 50 \n",
    "learning_rate = 0.1 \n",
    "decay_rate = learning_rate/epochs \n",
    "momentum = 0.8 \n",
    "sgd = SGD(lr=learning_rate, momentum=momentum, decay=decay_rate, nesterov=False) \n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "\n",
    "model.fit(X, encoded_y, validation_split=0.33, epochs=epochs, batch_size=28, verbose=2)\n",
    "# The model is trained on 67% of the dataset and evaluated using a 33% validation dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the example shows a classiﬁcation accuracy of 99.14%. This is higher than the baseline of 95.69% without the learning rate decay or momentum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop-Based Learning Rate Schedule\n",
    "Another popular learning rate schedule used with deep learning models is to systematically drop the learning rate at speciﬁc times during training. Often this method is implemented by dropping the learning rate by half every ﬁxed number of epochs. For example, we may have an initial learning rate of 0.1 and drop it by a factor of 0.5 every 10 epochs. The ﬁrst 10 epochs of training would use a value of 0.1, in the next 10 epochs a learning rate of 0.05 would be used, and so on. If we plot out the learning rates for this example out to 100 epochs...\n",
    "<img src=\"45.jpg\">\n",
    "\n",
    "We can implement this in Keras using the **LearningRateScheduler** callback when ﬁtting the model. It allows us to deﬁne a function to call that takes the **epoch number** as an argument and returns the **learning rate** to use in stochastic gradient descent. When used, the learning rate speciﬁed by stochastic gradient descent is ignored. In the code below, we use the same example as before of a single hidden layer network on the Ionosphere dataset. A new **`step_decay()`** function is deﬁned that implements the equation:\n",
    "\n",
    "$LearningRate = Initial LearningRate \\times DropRate ^ {floor(\\frac{1 + Epoch}{EpochDrop})}$ Where **InitialLearningRate** is the learning rate at the beginning of the run, **EpochDrop** is how often the learning rate is dropped in epochs and **DropRate** is how much to drop the learning rate each time it is dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.callbacks import LearningRateScheduler\n",
    "import math\n",
    "\n",
    "def step_decay(epoch): \n",
    "    initial_lrate = 0.1 \n",
    "    drop = 0.5 \n",
    "    epochs_drop = 10.0 \n",
    "    lrate = initial_lrate * math.pow(drop, math.floor((1+epoch)/epochs_drop)) \n",
    "    return(lrate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 235 samples, validate on 116 samples\n",
      "Epoch 1/50\n",
      " - 0s - loss: 0.0132 - acc: 0.9957 - val_loss: 0.0886 - val_acc: 0.9655\n",
      "Epoch 2/50\n",
      " - 0s - loss: 0.0174 - acc: 0.9957 - val_loss: 0.0808 - val_acc: 0.9655\n",
      "Epoch 3/50\n",
      " - 0s - loss: 0.0170 - acc: 0.9957 - val_loss: 0.0687 - val_acc: 0.9828\n",
      "Epoch 4/50\n",
      " - 0s - loss: 0.0249 - acc: 0.9957 - val_loss: 0.1020 - val_acc: 0.9655\n",
      "Epoch 5/50\n",
      " - 0s - loss: 0.0311 - acc: 0.9915 - val_loss: 0.0777 - val_acc: 0.9828\n",
      "Epoch 6/50\n",
      " - 0s - loss: 0.1003 - acc: 0.9660 - val_loss: 0.1816 - val_acc: 0.9310\n",
      "Epoch 7/50\n",
      " - 0s - loss: 0.0934 - acc: 0.9745 - val_loss: 0.1035 - val_acc: 0.9569\n",
      "Epoch 8/50\n",
      " - 0s - loss: 0.0567 - acc: 0.9745 - val_loss: 0.0728 - val_acc: 0.9828\n",
      "Epoch 9/50\n",
      " - 0s - loss: 0.0450 - acc: 0.9830 - val_loss: 0.1503 - val_acc: 0.9483\n",
      "Epoch 10/50\n",
      " - 0s - loss: 0.0377 - acc: 0.9830 - val_loss: 0.0860 - val_acc: 0.9741\n",
      "Epoch 11/50\n",
      " - 0s - loss: 0.0255 - acc: 0.9957 - val_loss: 0.0738 - val_acc: 0.9741\n",
      "Epoch 12/50\n",
      " - 0s - loss: 0.0267 - acc: 0.9957 - val_loss: 0.0873 - val_acc: 0.9741\n",
      "Epoch 13/50\n",
      " - 0s - loss: 0.0221 - acc: 0.9957 - val_loss: 0.0874 - val_acc: 0.9741\n",
      "Epoch 14/50\n",
      " - 0s - loss: 0.0203 - acc: 0.9957 - val_loss: 0.0986 - val_acc: 0.9741\n",
      "Epoch 15/50\n",
      " - 0s - loss: 0.0194 - acc: 0.9957 - val_loss: 0.0929 - val_acc: 0.9655\n",
      "Epoch 16/50\n",
      " - 0s - loss: 0.0189 - acc: 0.9957 - val_loss: 0.0903 - val_acc: 0.9741\n",
      "Epoch 17/50\n",
      " - 0s - loss: 0.0174 - acc: 0.9957 - val_loss: 0.1088 - val_acc: 0.9655\n",
      "Epoch 18/50\n",
      " - 0s - loss: 0.0173 - acc: 0.9957 - val_loss: 0.1035 - val_acc: 0.9655\n",
      "Epoch 19/50\n",
      " - 0s - loss: 0.0170 - acc: 0.9957 - val_loss: 0.0947 - val_acc: 0.9655\n",
      "Epoch 20/50\n",
      " - 0s - loss: 0.0161 - acc: 0.9957 - val_loss: 0.0989 - val_acc: 0.9655\n",
      "Epoch 21/50\n",
      " - 0s - loss: 0.0156 - acc: 0.9957 - val_loss: 0.0999 - val_acc: 0.9655\n",
      "Epoch 22/50\n",
      " - 0s - loss: 0.0158 - acc: 0.9957 - val_loss: 0.0976 - val_acc: 0.9655\n",
      "Epoch 23/50\n",
      " - 0s - loss: 0.0161 - acc: 0.9957 - val_loss: 0.1052 - val_acc: 0.9655\n",
      "Epoch 24/50\n",
      " - 0s - loss: 0.0154 - acc: 0.9957 - val_loss: 0.1042 - val_acc: 0.9655\n",
      "Epoch 25/50\n",
      " - 0s - loss: 0.0152 - acc: 0.9957 - val_loss: 0.0995 - val_acc: 0.9655\n",
      "Epoch 26/50\n",
      " - 0s - loss: 0.0152 - acc: 0.9957 - val_loss: 0.1022 - val_acc: 0.9655\n",
      "Epoch 27/50\n",
      " - 0s - loss: 0.0151 - acc: 0.9957 - val_loss: 0.1012 - val_acc: 0.9655\n",
      "Epoch 28/50\n",
      " - 0s - loss: 0.0152 - acc: 0.9957 - val_loss: 0.1046 - val_acc: 0.9655\n",
      "Epoch 29/50\n",
      " - 0s - loss: 0.0148 - acc: 0.9957 - val_loss: 0.1022 - val_acc: 0.9655\n",
      "Epoch 30/50\n",
      " - 0s - loss: 0.0144 - acc: 0.9957 - val_loss: 0.0998 - val_acc: 0.9569\n",
      "Epoch 31/50\n",
      " - 0s - loss: 0.0147 - acc: 0.9957 - val_loss: 0.0994 - val_acc: 0.9569\n",
      "Epoch 32/50\n",
      " - 0s - loss: 0.0149 - acc: 0.9957 - val_loss: 0.0984 - val_acc: 0.9569\n",
      "Epoch 33/50\n",
      " - 0s - loss: 0.0147 - acc: 0.9957 - val_loss: 0.1000 - val_acc: 0.9569\n",
      "Epoch 34/50\n",
      " - 0s - loss: 0.0141 - acc: 0.9957 - val_loss: 0.1131 - val_acc: 0.9655\n",
      "Epoch 35/50\n",
      " - 0s - loss: 0.0152 - acc: 0.9957 - val_loss: 0.1157 - val_acc: 0.9655\n",
      "Epoch 36/50\n",
      " - 0s - loss: 0.0148 - acc: 0.9957 - val_loss: 0.1102 - val_acc: 0.9655\n",
      "Epoch 37/50\n",
      " - 0s - loss: 0.0152 - acc: 0.9957 - val_loss: 0.0998 - val_acc: 0.9569\n",
      "Epoch 38/50\n",
      " - 0s - loss: 0.0146 - acc: 0.9957 - val_loss: 0.0988 - val_acc: 0.9569\n",
      "Epoch 39/50\n",
      " - 0s - loss: 0.0145 - acc: 0.9957 - val_loss: 0.0991 - val_acc: 0.9569\n",
      "Epoch 40/50\n",
      " - 0s - loss: 0.0142 - acc: 0.9957 - val_loss: 0.1019 - val_acc: 0.9569\n",
      "Epoch 41/50\n",
      " - 0s - loss: 0.0142 - acc: 0.9957 - val_loss: 0.1096 - val_acc: 0.9655\n",
      "Epoch 42/50\n",
      " - 0s - loss: 0.0143 - acc: 0.9957 - val_loss: 0.1113 - val_acc: 0.9655\n",
      "Epoch 43/50\n",
      " - 0s - loss: 0.0146 - acc: 0.9957 - val_loss: 0.1079 - val_acc: 0.9569\n",
      "Epoch 44/50\n",
      " - 0s - loss: 0.0142 - acc: 0.9957 - val_loss: 0.1074 - val_acc: 0.9569\n",
      "Epoch 45/50\n",
      " - 0s - loss: 0.0141 - acc: 0.9957 - val_loss: 0.1071 - val_acc: 0.9569\n",
      "Epoch 46/50\n",
      " - 0s - loss: 0.0141 - acc: 0.9957 - val_loss: 0.1056 - val_acc: 0.9569\n",
      "Epoch 47/50\n",
      " - 0s - loss: 0.0141 - acc: 0.9957 - val_loss: 0.1041 - val_acc: 0.9569\n",
      "Epoch 48/50\n",
      " - 0s - loss: 0.0141 - acc: 0.9957 - val_loss: 0.1040 - val_acc: 0.9569\n",
      "Epoch 49/50\n",
      " - 0s - loss: 0.0141 - acc: 0.9957 - val_loss: 0.1041 - val_acc: 0.9569\n",
      "Epoch 50/50\n",
      " - 0s - loss: 0.0140 - acc: 0.9957 - val_loss: 0.1040 - val_acc: 0.9569\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x22414a9a390>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compile model \n",
    "sgd = SGD(lr=0.0, momentum=0.9, decay=0.0, nesterov=False) \n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer=sgd, metrics=['accuracy']) \n",
    "# learning schedule callback \n",
    "lrate = LearningRateScheduler(step_decay) \n",
    "callbacks_list = [lrate] \n",
    "# Fit the model \n",
    "model.fit(X, encoded_y, validation_split=0.33, epochs=50, batch_size=28, callbacks=callbacks_list, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the example results in a classiﬁcation accuracy of 99.14% on the validation dataset, again an improvement over the baseline for the model on this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tips for Using Learning Rate Schedules\n",
    "- **Increase the initial learning rate.** Because the learning rate will decrease, start with a larger value to decrease from. A larger learning rate will result in much larger changes to the weights, at least in the beginning, allowing you to beneﬁt from ﬁne tuning later.\n",
    "- **Use a large momentum.** Using a larger momentum value will help the optimization algorithm continue to make updates in the right direction when your learning rate shrinks to small values.\n",
    "- **Experiment with diﬀerent schedules.** It will not be clear which learning rate schedule to use so try a few with diﬀerent conﬁguration options and see what works best on your problem. Also try schedules that change exponentially and even schedules that respond to the accuracy of your model on the training or test datasets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
