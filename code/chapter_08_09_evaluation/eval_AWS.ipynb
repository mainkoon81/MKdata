{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "There are a myriad of decisions you must make when conﬁguring your deep learning models. Many of these decisions can be resolved by copying the structure of other people’s networks and using heuristics. Ultimately, the best technique is to actually design small experiments and empirically evaluate options using real data. This includes:\n",
    "- high-level decisions\n",
    "  - the number, size and type of layers in your network. \n",
    "- lower-level decisions \n",
    "  - the choice of **loss** function, **activation** functions, **optimization** procedure and number of **epochs**.\n",
    "  \n",
    "As such, you need to have a robust test harness that allows you to estimate the performance of a given conﬁguration on unseen data, and reliably compare the performance to other conﬁgurations.\n",
    "\n",
    "**Splitting:** The large amount of data and the complexity of the models require very long training times. As such, it is typical to use a simple separation of data into training and test(validation) datasets. Keras provides two convenient ways of evaluating your deep learning algorithms this way:\n",
    "1. Use an automatic veriﬁcation dataset.\n",
    "2. Use a manual veriﬁcation dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Automatic verification dataset\n",
    "Keras can separate a portion of your **training data** into a **validation data** and evaluate the performance of your model on that validation dataset **each epoch** by setting the validation split argument on the **`fit()`** function to a percentage of the size of your training dataset.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('pima-indians-diabetes.csv', header=None)\n",
    "#X = df[:][0:7] it won't work..[row][fucked] coz it's a series..the multi-col is not allowed.\n",
    "#y = df[:][8] it works..[row][col] coz it's a series..\n",
    "\n",
    "X = df.iloc[:, 0:8] #dataframe\n",
    "y = df.iloc[:, 8] #series\n",
    "\n",
    "# df.shape : 768 x 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(47)\n",
    "\n",
    "data = np.loadtxt('pima-indians-diabetes.csv', delimiter=',')\n",
    "X = data[:, 0:8] #array\n",
    "y = data[:,8] #array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 514 samples, validate on 254 samples\n",
      "Epoch 1/150\n",
      "514/514 [==============================] - 0s 811us/step - loss: 10.0003 - acc: 0.3599 - val_loss: 9.6643 - val_acc: 0.3268\n",
      "Epoch 2/150\n",
      "514/514 [==============================] - 0s 163us/step - loss: 4.9936 - acc: 0.3735 - val_loss: 2.1213 - val_acc: 0.4843\n",
      "Epoch 3/150\n",
      "514/514 [==============================] - 0s 160us/step - loss: 1.2415 - acc: 0.4630 - val_loss: 0.8225 - val_acc: 0.5276\n",
      "Epoch 4/150\n",
      "514/514 [==============================] - 0s 161us/step - loss: 0.7207 - acc: 0.5642 - val_loss: 0.7049 - val_acc: 0.6024\n",
      "Epoch 5/150\n",
      "514/514 [==============================] - 0s 161us/step - loss: 0.6581 - acc: 0.6401 - val_loss: 0.6873 - val_acc: 0.6024\n",
      "Epoch 6/150\n",
      "514/514 [==============================] - 0s 163us/step - loss: 0.6479 - acc: 0.6342 - val_loss: 0.6765 - val_acc: 0.6102\n",
      "Epoch 7/150\n",
      "514/514 [==============================] - 0s 161us/step - loss: 0.6411 - acc: 0.6342 - val_loss: 0.7005 - val_acc: 0.5591\n",
      "Epoch 8/150\n",
      "514/514 [==============================] - 0s 163us/step - loss: 0.6340 - acc: 0.6537 - val_loss: 0.6696 - val_acc: 0.6181\n",
      "Epoch 9/150\n",
      "514/514 [==============================] - 0s 164us/step - loss: 0.6296 - acc: 0.6556 - val_loss: 0.6619 - val_acc: 0.6457\n",
      "Epoch 10/150\n",
      "514/514 [==============================] - 0s 162us/step - loss: 0.6309 - acc: 0.6556 - val_loss: 0.6563 - val_acc: 0.6417\n",
      "Epoch 11/150\n",
      "514/514 [==============================] - 0s 161us/step - loss: 0.6271 - acc: 0.6518 - val_loss: 0.6518 - val_acc: 0.6260\n",
      "Epoch 12/150\n",
      "514/514 [==============================] - 0s 159us/step - loss: 0.6205 - acc: 0.6712 - val_loss: 0.6501 - val_acc: 0.6260\n",
      "Epoch 13/150\n",
      "514/514 [==============================] - 0s 160us/step - loss: 0.6175 - acc: 0.6654 - val_loss: 0.6445 - val_acc: 0.6417\n",
      "Epoch 14/150\n",
      "514/514 [==============================] - 0s 159us/step - loss: 0.6166 - acc: 0.6751 - val_loss: 0.6402 - val_acc: 0.6535\n",
      "Epoch 15/150\n",
      "514/514 [==============================] - 0s 161us/step - loss: 0.6123 - acc: 0.6693 - val_loss: 0.6453 - val_acc: 0.6181\n",
      "Epoch 16/150\n",
      "514/514 [==============================] - 0s 160us/step - loss: 0.6224 - acc: 0.6732 - val_loss: 0.6429 - val_acc: 0.6496\n",
      "Epoch 17/150\n",
      "514/514 [==============================] - 0s 160us/step - loss: 0.6092 - acc: 0.6965 - val_loss: 0.6388 - val_acc: 0.6181\n",
      "Epoch 18/150\n",
      "514/514 [==============================] - 0s 160us/step - loss: 0.6066 - acc: 0.6809 - val_loss: 0.6301 - val_acc: 0.6614\n",
      "Epoch 19/150\n",
      "514/514 [==============================] - 0s 159us/step - loss: 0.6080 - acc: 0.6790 - val_loss: 0.6335 - val_acc: 0.6260\n",
      "Epoch 20/150\n",
      "514/514 [==============================] - 0s 160us/step - loss: 0.5966 - acc: 0.7043 - val_loss: 0.6314 - val_acc: 0.6260\n",
      "Epoch 21/150\n",
      "514/514 [==============================] - 0s 162us/step - loss: 0.5941 - acc: 0.7062 - val_loss: 0.6416 - val_acc: 0.6339\n",
      "Epoch 22/150\n",
      "514/514 [==============================] - 0s 162us/step - loss: 0.6100 - acc: 0.6829 - val_loss: 0.6196 - val_acc: 0.6614\n",
      "Epoch 23/150\n",
      "514/514 [==============================] - 0s 159us/step - loss: 0.5950 - acc: 0.6984 - val_loss: 0.6203 - val_acc: 0.6654\n",
      "Epoch 24/150\n",
      "514/514 [==============================] - 0s 161us/step - loss: 0.5911 - acc: 0.7082 - val_loss: 0.6213 - val_acc: 0.6378\n",
      "Epoch 25/150\n",
      "514/514 [==============================] - 0s 159us/step - loss: 0.5876 - acc: 0.7179 - val_loss: 0.6245 - val_acc: 0.6299\n",
      "Epoch 26/150\n",
      "514/514 [==============================] - 0s 161us/step - loss: 0.5885 - acc: 0.6965 - val_loss: 0.6160 - val_acc: 0.6496\n",
      "Epoch 27/150\n",
      "514/514 [==============================] - 0s 160us/step - loss: 0.5838 - acc: 0.7101 - val_loss: 0.6154 - val_acc: 0.6654\n",
      "Epoch 28/150\n",
      "514/514 [==============================] - 0s 158us/step - loss: 0.5878 - acc: 0.6984 - val_loss: 0.6134 - val_acc: 0.6535\n",
      "Epoch 29/150\n",
      "514/514 [==============================] - 0s 161us/step - loss: 0.5840 - acc: 0.6965 - val_loss: 0.6174 - val_acc: 0.6772\n",
      "Epoch 30/150\n",
      "514/514 [==============================] - 0s 160us/step - loss: 0.5798 - acc: 0.6984 - val_loss: 0.6187 - val_acc: 0.6535\n",
      "Epoch 31/150\n",
      "514/514 [==============================] - 0s 160us/step - loss: 0.5793 - acc: 0.7082 - val_loss: 0.6030 - val_acc: 0.6890\n",
      "Epoch 32/150\n",
      "514/514 [==============================] - 0s 162us/step - loss: 0.5823 - acc: 0.7004 - val_loss: 0.6040 - val_acc: 0.6772\n",
      "Epoch 33/150\n",
      "514/514 [==============================] - 0s 159us/step - loss: 0.5762 - acc: 0.7062 - val_loss: 0.5981 - val_acc: 0.6772\n",
      "Epoch 34/150\n",
      "514/514 [==============================] - 0s 161us/step - loss: 0.5714 - acc: 0.7121 - val_loss: 0.5981 - val_acc: 0.6890\n",
      "Epoch 35/150\n",
      "514/514 [==============================] - 0s 160us/step - loss: 0.5701 - acc: 0.7140 - val_loss: 0.5962 - val_acc: 0.6772\n",
      "Epoch 36/150\n",
      "514/514 [==============================] - 0s 161us/step - loss: 0.5676 - acc: 0.7198 - val_loss: 0.5939 - val_acc: 0.6929\n",
      "Epoch 37/150\n",
      "514/514 [==============================] - 0s 160us/step - loss: 0.5653 - acc: 0.7198 - val_loss: 0.5936 - val_acc: 0.6850\n",
      "Epoch 38/150\n",
      "514/514 [==============================] - 0s 161us/step - loss: 0.5615 - acc: 0.7198 - val_loss: 0.5923 - val_acc: 0.6732\n",
      "Epoch 39/150\n",
      "514/514 [==============================] - 0s 164us/step - loss: 0.5730 - acc: 0.7237 - val_loss: 0.5933 - val_acc: 0.6890\n",
      "Epoch 40/150\n",
      "514/514 [==============================] - 0s 162us/step - loss: 0.5783 - acc: 0.7062 - val_loss: 0.6081 - val_acc: 0.6850\n",
      "Epoch 41/150\n",
      "514/514 [==============================] - 0s 160us/step - loss: 0.5646 - acc: 0.7198 - val_loss: 0.5938 - val_acc: 0.6811\n",
      "Epoch 42/150\n",
      "514/514 [==============================] - 0s 160us/step - loss: 0.5596 - acc: 0.7043 - val_loss: 0.5970 - val_acc: 0.6969\n",
      "Epoch 43/150\n",
      "514/514 [==============================] - 0s 160us/step - loss: 0.5619 - acc: 0.7101 - val_loss: 0.5972 - val_acc: 0.7047\n",
      "Epoch 44/150\n",
      "514/514 [==============================] - 0s 160us/step - loss: 0.5719 - acc: 0.7043 - val_loss: 0.5830 - val_acc: 0.7008\n",
      "Epoch 45/150\n",
      "514/514 [==============================] - 0s 160us/step - loss: 0.5536 - acc: 0.7140 - val_loss: 0.5816 - val_acc: 0.6969\n",
      "Epoch 46/150\n",
      "514/514 [==============================] - 0s 163us/step - loss: 0.5552 - acc: 0.7140 - val_loss: 0.5825 - val_acc: 0.7087\n",
      "Epoch 47/150\n",
      "514/514 [==============================] - 0s 159us/step - loss: 0.5526 - acc: 0.7179 - val_loss: 0.5759 - val_acc: 0.7087\n",
      "Epoch 48/150\n",
      "514/514 [==============================] - 0s 158us/step - loss: 0.5528 - acc: 0.7179 - val_loss: 0.5767 - val_acc: 0.7126\n",
      "Epoch 49/150\n",
      "514/514 [==============================] - 0s 159us/step - loss: 0.5551 - acc: 0.7276 - val_loss: 0.5798 - val_acc: 0.7087\n",
      "Epoch 50/150\n",
      "514/514 [==============================] - 0s 162us/step - loss: 0.5514 - acc: 0.7237 - val_loss: 0.5769 - val_acc: 0.7087\n",
      "Epoch 51/150\n",
      "514/514 [==============================] - 0s 161us/step - loss: 0.5516 - acc: 0.7237 - val_loss: 0.5844 - val_acc: 0.7008\n",
      "Epoch 52/150\n",
      "514/514 [==============================] - 0s 161us/step - loss: 0.5509 - acc: 0.7198 - val_loss: 0.5764 - val_acc: 0.7008\n",
      "Epoch 53/150\n",
      "514/514 [==============================] - 0s 160us/step - loss: 0.5458 - acc: 0.7101 - val_loss: 0.5720 - val_acc: 0.7087\n",
      "Epoch 54/150\n",
      "514/514 [==============================] - 0s 161us/step - loss: 0.5480 - acc: 0.7315 - val_loss: 0.5715 - val_acc: 0.7126\n",
      "Epoch 55/150\n",
      "514/514 [==============================] - 0s 160us/step - loss: 0.5458 - acc: 0.7198 - val_loss: 0.5734 - val_acc: 0.7047\n",
      "Epoch 56/150\n",
      "514/514 [==============================] - 0s 160us/step - loss: 0.5424 - acc: 0.7335 - val_loss: 0.5705 - val_acc: 0.7126\n",
      "Epoch 57/150\n",
      "514/514 [==============================] - 0s 161us/step - loss: 0.5475 - acc: 0.6984 - val_loss: 0.5768 - val_acc: 0.6969\n",
      "Epoch 58/150\n",
      "514/514 [==============================] - 0s 160us/step - loss: 0.5459 - acc: 0.7335 - val_loss: 0.5684 - val_acc: 0.7283\n",
      "Epoch 59/150\n",
      "514/514 [==============================] - 0s 162us/step - loss: 0.5395 - acc: 0.7374 - val_loss: 0.5932 - val_acc: 0.7047\n",
      "Epoch 60/150\n",
      "514/514 [==============================] - 0s 160us/step - loss: 0.5394 - acc: 0.7374 - val_loss: 0.5818 - val_acc: 0.6969\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 61/150\n",
      "514/514 [==============================] - 0s 160us/step - loss: 0.5387 - acc: 0.7218 - val_loss: 0.5762 - val_acc: 0.7087\n",
      "Epoch 62/150\n",
      "514/514 [==============================] - 0s 160us/step - loss: 0.5462 - acc: 0.7218 - val_loss: 0.5712 - val_acc: 0.7008\n",
      "Epoch 63/150\n",
      "514/514 [==============================] - 0s 159us/step - loss: 0.5506 - acc: 0.7276 - val_loss: 0.5699 - val_acc: 0.7047\n",
      "Epoch 64/150\n",
      "514/514 [==============================] - 0s 160us/step - loss: 0.5430 - acc: 0.7296 - val_loss: 0.5960 - val_acc: 0.7008\n",
      "Epoch 65/150\n",
      "514/514 [==============================] - 0s 160us/step - loss: 0.5426 - acc: 0.7179 - val_loss: 0.5631 - val_acc: 0.7205\n",
      "Epoch 66/150\n",
      "514/514 [==============================] - 0s 159us/step - loss: 0.5366 - acc: 0.7296 - val_loss: 0.5620 - val_acc: 0.7244\n",
      "Epoch 67/150\n",
      "514/514 [==============================] - 0s 161us/step - loss: 0.5362 - acc: 0.7237 - val_loss: 0.5628 - val_acc: 0.7047\n",
      "Epoch 68/150\n",
      "514/514 [==============================] - 0s 160us/step - loss: 0.5282 - acc: 0.7354 - val_loss: 0.5624 - val_acc: 0.7165\n",
      "Epoch 69/150\n",
      "514/514 [==============================] - 0s 160us/step - loss: 0.5434 - acc: 0.7160 - val_loss: 0.5595 - val_acc: 0.7087\n",
      "Epoch 70/150\n",
      "514/514 [==============================] - 0s 161us/step - loss: 0.5297 - acc: 0.7315 - val_loss: 0.5654 - val_acc: 0.7165\n",
      "Epoch 71/150\n",
      "514/514 [==============================] - 0s 161us/step - loss: 0.5308 - acc: 0.7374 - val_loss: 0.5660 - val_acc: 0.7205\n",
      "Epoch 72/150\n",
      "514/514 [==============================] - 0s 160us/step - loss: 0.5267 - acc: 0.7296 - val_loss: 0.5588 - val_acc: 0.7008\n",
      "Epoch 73/150\n",
      "514/514 [==============================] - 0s 160us/step - loss: 0.5384 - acc: 0.7412 - val_loss: 0.5680 - val_acc: 0.6969\n",
      "Epoch 74/150\n",
      "514/514 [==============================] - 0s 160us/step - loss: 0.5292 - acc: 0.7354 - val_loss: 0.5569 - val_acc: 0.7126\n",
      "Epoch 75/150\n",
      "514/514 [==============================] - 0s 160us/step - loss: 0.5285 - acc: 0.7374 - val_loss: 0.5558 - val_acc: 0.7362\n",
      "Epoch 76/150\n",
      "514/514 [==============================] - 0s 159us/step - loss: 0.5259 - acc: 0.7198 - val_loss: 0.5576 - val_acc: 0.7362\n",
      "Epoch 77/150\n",
      "514/514 [==============================] - 0s 160us/step - loss: 0.5358 - acc: 0.7257 - val_loss: 0.5690 - val_acc: 0.7283\n",
      "Epoch 78/150\n",
      "514/514 [==============================] - 0s 159us/step - loss: 0.5260 - acc: 0.7412 - val_loss: 0.5649 - val_acc: 0.7087\n",
      "Epoch 79/150\n",
      "514/514 [==============================] - 0s 157us/step - loss: 0.5218 - acc: 0.7315 - val_loss: 0.5568 - val_acc: 0.7087\n",
      "Epoch 80/150\n",
      "514/514 [==============================] - 0s 161us/step - loss: 0.5313 - acc: 0.7276 - val_loss: 0.5573 - val_acc: 0.7205\n",
      "Epoch 81/150\n",
      "514/514 [==============================] - 0s 160us/step - loss: 0.5295 - acc: 0.7451 - val_loss: 0.5645 - val_acc: 0.7205\n",
      "Epoch 82/150\n",
      "514/514 [==============================] - 0s 162us/step - loss: 0.5285 - acc: 0.7412 - val_loss: 0.5617 - val_acc: 0.7165\n",
      "Epoch 83/150\n",
      "514/514 [==============================] - 0s 163us/step - loss: 0.5278 - acc: 0.7354 - val_loss: 0.5560 - val_acc: 0.7087\n",
      "Epoch 84/150\n",
      "514/514 [==============================] - 0s 162us/step - loss: 0.5207 - acc: 0.7490 - val_loss: 0.5540 - val_acc: 0.7165\n",
      "Epoch 85/150\n",
      "514/514 [==============================] - 0s 159us/step - loss: 0.5198 - acc: 0.7315 - val_loss: 0.5554 - val_acc: 0.7244\n",
      "Epoch 86/150\n",
      "514/514 [==============================] - 0s 157us/step - loss: 0.5174 - acc: 0.7354 - val_loss: 0.5548 - val_acc: 0.6850\n",
      "Epoch 87/150\n",
      "514/514 [==============================] - 0s 165us/step - loss: 0.5175 - acc: 0.7393 - val_loss: 0.5763 - val_acc: 0.7087\n",
      "Epoch 88/150\n",
      "514/514 [==============================] - 0s 161us/step - loss: 0.5184 - acc: 0.7393 - val_loss: 0.5669 - val_acc: 0.7244\n",
      "Epoch 89/150\n",
      "514/514 [==============================] - 0s 160us/step - loss: 0.5176 - acc: 0.7451 - val_loss: 0.5781 - val_acc: 0.7008\n",
      "Epoch 90/150\n",
      "514/514 [==============================] - 0s 161us/step - loss: 0.5265 - acc: 0.7296 - val_loss: 0.5540 - val_acc: 0.7165\n",
      "Epoch 91/150\n",
      "514/514 [==============================] - 0s 161us/step - loss: 0.5158 - acc: 0.7490 - val_loss: 0.5541 - val_acc: 0.7283\n",
      "Epoch 92/150\n",
      "514/514 [==============================] - 0s 160us/step - loss: 0.5274 - acc: 0.7432 - val_loss: 0.5548 - val_acc: 0.7126\n",
      "Epoch 93/150\n",
      "514/514 [==============================] - 0s 161us/step - loss: 0.5193 - acc: 0.7432 - val_loss: 0.5529 - val_acc: 0.7047\n",
      "Epoch 94/150\n",
      "514/514 [==============================] - 0s 162us/step - loss: 0.5116 - acc: 0.7393 - val_loss: 0.5707 - val_acc: 0.7047\n",
      "Epoch 95/150\n",
      "514/514 [==============================] - 0s 159us/step - loss: 0.5140 - acc: 0.7432 - val_loss: 0.5703 - val_acc: 0.7087\n",
      "Epoch 96/150\n",
      "514/514 [==============================] - 0s 159us/step - loss: 0.5187 - acc: 0.7393 - val_loss: 0.5636 - val_acc: 0.7244\n",
      "Epoch 97/150\n",
      "514/514 [==============================] - 0s 161us/step - loss: 0.5144 - acc: 0.7471 - val_loss: 0.5606 - val_acc: 0.7205\n",
      "Epoch 98/150\n",
      "514/514 [==============================] - 0s 159us/step - loss: 0.5068 - acc: 0.7471 - val_loss: 0.5559 - val_acc: 0.7008\n",
      "Epoch 99/150\n",
      "514/514 [==============================] - 0s 160us/step - loss: 0.5105 - acc: 0.7451 - val_loss: 0.5756 - val_acc: 0.7244\n",
      "Epoch 100/150\n",
      "514/514 [==============================] - 0s 161us/step - loss: 0.5089 - acc: 0.7510 - val_loss: 0.5616 - val_acc: 0.7165\n",
      "Epoch 101/150\n",
      "514/514 [==============================] - 0s 160us/step - loss: 0.5100 - acc: 0.7510 - val_loss: 0.5626 - val_acc: 0.7323\n",
      "Epoch 102/150\n",
      "514/514 [==============================] - 0s 160us/step - loss: 0.5111 - acc: 0.7412 - val_loss: 0.5553 - val_acc: 0.7362\n",
      "Epoch 103/150\n",
      "514/514 [==============================] - 0s 160us/step - loss: 0.5076 - acc: 0.7432 - val_loss: 0.5588 - val_acc: 0.7165\n",
      "Epoch 104/150\n",
      "514/514 [==============================] - 0s 161us/step - loss: 0.5085 - acc: 0.7549 - val_loss: 0.5578 - val_acc: 0.7402\n",
      "Epoch 105/150\n",
      "514/514 [==============================] - 0s 162us/step - loss: 0.5105 - acc: 0.7451 - val_loss: 0.5576 - val_acc: 0.7205\n",
      "Epoch 106/150\n",
      "514/514 [==============================] - 0s 161us/step - loss: 0.5153 - acc: 0.7393 - val_loss: 0.5643 - val_acc: 0.7362\n",
      "Epoch 107/150\n",
      "514/514 [==============================] - 0s 164us/step - loss: 0.5119 - acc: 0.7412 - val_loss: 0.5630 - val_acc: 0.7008\n",
      "Epoch 108/150\n",
      "514/514 [==============================] - 0s 161us/step - loss: 0.5160 - acc: 0.7471 - val_loss: 0.5600 - val_acc: 0.7165\n",
      "Epoch 109/150\n",
      "514/514 [==============================] - 0s 163us/step - loss: 0.5136 - acc: 0.7510 - val_loss: 0.5550 - val_acc: 0.7323\n",
      "Epoch 110/150\n",
      "514/514 [==============================] - 0s 164us/step - loss: 0.5093 - acc: 0.7568 - val_loss: 0.5537 - val_acc: 0.7244\n",
      "Epoch 111/150\n",
      "514/514 [==============================] - 0s 158us/step - loss: 0.5062 - acc: 0.7510 - val_loss: 0.5544 - val_acc: 0.7126\n",
      "Epoch 112/150\n",
      "514/514 [==============================] - 0s 162us/step - loss: 0.5134 - acc: 0.7549 - val_loss: 0.5609 - val_acc: 0.7402\n",
      "Epoch 113/150\n",
      "514/514 [==============================] - 0s 161us/step - loss: 0.5036 - acc: 0.7549 - val_loss: 0.5596 - val_acc: 0.7283\n",
      "Epoch 114/150\n",
      "514/514 [==============================] - 0s 159us/step - loss: 0.5141 - acc: 0.7393 - val_loss: 0.5573 - val_acc: 0.7165\n",
      "Epoch 115/150\n",
      "514/514 [==============================] - 0s 159us/step - loss: 0.5050 - acc: 0.7490 - val_loss: 0.5529 - val_acc: 0.7047\n",
      "Epoch 116/150\n",
      "514/514 [==============================] - 0s 159us/step - loss: 0.5030 - acc: 0.7568 - val_loss: 0.5572 - val_acc: 0.7480\n",
      "Epoch 117/150\n",
      "514/514 [==============================] - 0s 157us/step - loss: 0.5044 - acc: 0.7529 - val_loss: 0.5600 - val_acc: 0.7441\n",
      "Epoch 118/150\n",
      "514/514 [==============================] - 0s 157us/step - loss: 0.5069 - acc: 0.7626 - val_loss: 0.5613 - val_acc: 0.7244\n",
      "Epoch 119/150\n",
      "514/514 [==============================] - 0s 161us/step - loss: 0.5065 - acc: 0.7646 - val_loss: 0.5557 - val_acc: 0.7087\n",
      "Epoch 120/150\n",
      "514/514 [==============================] - 0s 163us/step - loss: 0.5044 - acc: 0.7529 - val_loss: 0.5620 - val_acc: 0.7402\n",
      "Epoch 121/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "514/514 [==============================] - 0s 160us/step - loss: 0.5102 - acc: 0.7490 - val_loss: 0.5501 - val_acc: 0.7283\n",
      "Epoch 122/150\n",
      "514/514 [==============================] - 0s 158us/step - loss: 0.4990 - acc: 0.7529 - val_loss: 0.5578 - val_acc: 0.7126\n",
      "Epoch 123/150\n",
      "514/514 [==============================] - 0s 160us/step - loss: 0.5087 - acc: 0.7529 - val_loss: 0.5783 - val_acc: 0.7087\n",
      "Epoch 124/150\n",
      "514/514 [==============================] - 0s 161us/step - loss: 0.5064 - acc: 0.7510 - val_loss: 0.5704 - val_acc: 0.7087\n",
      "Epoch 125/150\n",
      "514/514 [==============================] - 0s 160us/step - loss: 0.5347 - acc: 0.7354 - val_loss: 0.5553 - val_acc: 0.7362\n",
      "Epoch 126/150\n",
      "514/514 [==============================] - 0s 161us/step - loss: 0.4994 - acc: 0.7665 - val_loss: 0.5553 - val_acc: 0.6929\n",
      "Epoch 127/150\n",
      "514/514 [==============================] - 0s 159us/step - loss: 0.4973 - acc: 0.7588 - val_loss: 0.5587 - val_acc: 0.7362\n",
      "Epoch 128/150\n",
      "514/514 [==============================] - 0s 159us/step - loss: 0.5061 - acc: 0.7549 - val_loss: 0.5646 - val_acc: 0.7008\n",
      "Epoch 129/150\n",
      "514/514 [==============================] - 0s 160us/step - loss: 0.5043 - acc: 0.7588 - val_loss: 0.5610 - val_acc: 0.7165\n",
      "Epoch 130/150\n",
      "514/514 [==============================] - 0s 160us/step - loss: 0.5020 - acc: 0.7471 - val_loss: 0.5629 - val_acc: 0.7244\n",
      "Epoch 131/150\n",
      "514/514 [==============================] - 0s 162us/step - loss: 0.5054 - acc: 0.7549 - val_loss: 0.5677 - val_acc: 0.7362\n",
      "Epoch 132/150\n",
      "514/514 [==============================] - 0s 160us/step - loss: 0.4996 - acc: 0.7510 - val_loss: 0.5545 - val_acc: 0.7520\n",
      "Epoch 133/150\n",
      "514/514 [==============================] - 0s 160us/step - loss: 0.4973 - acc: 0.7665 - val_loss: 0.5549 - val_acc: 0.7362\n",
      "Epoch 134/150\n",
      "514/514 [==============================] - 0s 162us/step - loss: 0.5044 - acc: 0.7607 - val_loss: 0.5494 - val_acc: 0.7205\n",
      "Epoch 135/150\n",
      "514/514 [==============================] - 0s 160us/step - loss: 0.4971 - acc: 0.7549 - val_loss: 0.5797 - val_acc: 0.7165\n",
      "Epoch 136/150\n",
      "514/514 [==============================] - 0s 159us/step - loss: 0.5039 - acc: 0.7529 - val_loss: 0.5463 - val_acc: 0.7323\n",
      "Epoch 137/150\n",
      "514/514 [==============================] - 0s 159us/step - loss: 0.5059 - acc: 0.7451 - val_loss: 0.5620 - val_acc: 0.7244\n",
      "Epoch 138/150\n",
      "514/514 [==============================] - 0s 157us/step - loss: 0.4918 - acc: 0.7412 - val_loss: 0.5704 - val_acc: 0.7087\n",
      "Epoch 139/150\n",
      "514/514 [==============================] - 0s 158us/step - loss: 0.5032 - acc: 0.7568 - val_loss: 0.5548 - val_acc: 0.7480\n",
      "Epoch 140/150\n",
      "514/514 [==============================] - 0s 160us/step - loss: 0.5099 - acc: 0.7374 - val_loss: 0.5555 - val_acc: 0.7362\n",
      "Epoch 141/150\n",
      "514/514 [==============================] - 0s 160us/step - loss: 0.5034 - acc: 0.7412 - val_loss: 0.5795 - val_acc: 0.7008\n",
      "Epoch 142/150\n",
      "514/514 [==============================] - 0s 157us/step - loss: 0.4966 - acc: 0.7510 - val_loss: 0.5539 - val_acc: 0.7323\n",
      "Epoch 143/150\n",
      "514/514 [==============================] - 0s 159us/step - loss: 0.4997 - acc: 0.7568 - val_loss: 0.5513 - val_acc: 0.7441\n",
      "Epoch 144/150\n",
      "514/514 [==============================] - 0s 161us/step - loss: 0.4974 - acc: 0.7568 - val_loss: 0.5523 - val_acc: 0.7283\n",
      "Epoch 145/150\n",
      "514/514 [==============================] - 0s 160us/step - loss: 0.4957 - acc: 0.7549 - val_loss: 0.5518 - val_acc: 0.7283\n",
      "Epoch 146/150\n",
      "514/514 [==============================] - 0s 159us/step - loss: 0.4909 - acc: 0.7588 - val_loss: 0.5524 - val_acc: 0.7244\n",
      "Epoch 147/150\n",
      "514/514 [==============================] - 0s 160us/step - loss: 0.5034 - acc: 0.7432 - val_loss: 0.5493 - val_acc: 0.7323\n",
      "Epoch 148/150\n",
      "514/514 [==============================] - 0s 160us/step - loss: 0.4885 - acc: 0.7588 - val_loss: 0.5735 - val_acc: 0.7165\n",
      "Epoch 149/150\n",
      "514/514 [==============================] - 0s 162us/step - loss: 0.4991 - acc: 0.7665 - val_loss: 0.5635 - val_acc: 0.7244\n",
      "Epoch 150/150\n",
      "514/514 [==============================] - 0s 161us/step - loss: 0.4916 - acc: 0.7607 - val_loss: 0.5562 - val_acc: 0.7402\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f25f5a909e8>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(12, input_dim=8, activation='relu'))\n",
    "model.add(Dense(8, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model.fit(X, y, validation_split=0.33, epochs=150, batch_size=10)\n",
    "# batch_size defines number of samples that going to be propagated through the network.\n",
    "#here, Algorithm takes first 10 samples (from 1st to 10th) from the training dataset and trains network\n",
    "#(that are evaluated before a weight update in the network). Next it takes second 10\n",
    "#samples (from 11st to 20th) and train network again. We can keep doing this procedure until we will propagate through the \n",
    "#networks all samples. The problem usually happens with the last set of samples. In our example we've used 768 which is not \n",
    "#divisible by 10 without remainder. The simplest solution is just to get final 8 samples and train the network.\n",
    "# (+)Typically networks trains faster with mini-batches. That's because we update weights after each propagation.\n",
    "# (-)The smaller the batch the less accurate estimate of the gradient.\n",
    "\n",
    "# 150 epoch = 150 forward pass and 150 backward pass of all the training examples.(licking)\n",
    "# batch size = the number of training examples in one forward/backward pass. (one dish)\n",
    "# number of iterations = number of passes, each pass using [batch size] number of examples. (size of dishes)\n",
    "\n",
    "#To be clear, one pass = one forward pass + one backward pass (we do not count the forward pass and backward pass as two \n",
    "#different passes)..for Example: if you have 1000 training examples, and your batch size is 500, then it will take 2 iterations.\n",
    "\n",
    "#you can see that the verbose output on each epoch that shows the loss and accuracy on both the training dataset(loss, acc) and\n",
    "#the validation dataset(val_loss, val_acc)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Manual verification dataset\n",
    "In this example we use the handy **`train_test_split()`** function from the scikit-learn library to separate our data into a training and test dataset. The validation dataset can be speciﬁed to the **`fit()`** function in Keras by the **validation_data** argument. It takes a tuple of the input and output datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 514 samples, validate on 254 samples\n",
      "Epoch 1/150\n",
      "514/514 [==============================] - 0s 631us/step - loss: 1.8926 - acc: 0.4903 - val_loss: 1.2074 - val_acc: 0.5197\n",
      "Epoch 2/150\n",
      "514/514 [==============================] - 0s 161us/step - loss: 0.9594 - acc: 0.5856 - val_loss: 0.8645 - val_acc: 0.5315\n",
      "Epoch 3/150\n",
      "514/514 [==============================] - 0s 161us/step - loss: 0.7200 - acc: 0.6479 - val_loss: 0.7538 - val_acc: 0.5984\n",
      "Epoch 4/150\n",
      "514/514 [==============================] - 0s 159us/step - loss: 0.6505 - acc: 0.6673 - val_loss: 0.7154 - val_acc: 0.5945\n",
      "Epoch 5/150\n",
      "514/514 [==============================] - 0s 163us/step - loss: 0.6447 - acc: 0.6693 - val_loss: 0.7075 - val_acc: 0.6417\n",
      "Epoch 6/150\n",
      "514/514 [==============================] - 0s 160us/step - loss: 0.6285 - acc: 0.6809 - val_loss: 0.6878 - val_acc: 0.6614\n",
      "Epoch 7/150\n",
      "514/514 [==============================] - 0s 160us/step - loss: 0.6267 - acc: 0.6790 - val_loss: 0.6841 - val_acc: 0.6260\n",
      "Epoch 8/150\n",
      "514/514 [==============================] - 0s 161us/step - loss: 0.6118 - acc: 0.6732 - val_loss: 0.6788 - val_acc: 0.6614\n",
      "Epoch 9/150\n",
      "514/514 [==============================] - 0s 163us/step - loss: 0.6051 - acc: 0.6848 - val_loss: 0.6866 - val_acc: 0.6772\n",
      "Epoch 10/150\n",
      "514/514 [==============================] - 0s 159us/step - loss: 0.6044 - acc: 0.7062 - val_loss: 0.6583 - val_acc: 0.6654\n",
      "Epoch 11/150\n",
      "514/514 [==============================] - 0s 160us/step - loss: 0.5859 - acc: 0.7062 - val_loss: 0.6488 - val_acc: 0.6575\n",
      "Epoch 12/150\n",
      "514/514 [==============================] - 0s 160us/step - loss: 0.6039 - acc: 0.7004 - val_loss: 0.6451 - val_acc: 0.6850\n",
      "Epoch 13/150\n",
      "514/514 [==============================] - 0s 162us/step - loss: 0.5847 - acc: 0.7043 - val_loss: 0.6330 - val_acc: 0.6929\n",
      "Epoch 14/150\n",
      "514/514 [==============================] - 0s 161us/step - loss: 0.5796 - acc: 0.7004 - val_loss: 0.6351 - val_acc: 0.6929\n",
      "Epoch 15/150\n",
      "514/514 [==============================] - 0s 159us/step - loss: 0.5767 - acc: 0.7121 - val_loss: 0.6507 - val_acc: 0.6575\n",
      "Epoch 16/150\n",
      "514/514 [==============================] - 0s 160us/step - loss: 0.5754 - acc: 0.7004 - val_loss: 0.6388 - val_acc: 0.6850\n",
      "Epoch 17/150\n",
      "514/514 [==============================] - 0s 157us/step - loss: 0.5784 - acc: 0.6965 - val_loss: 0.6452 - val_acc: 0.6811\n",
      "Epoch 18/150\n",
      "514/514 [==============================] - 0s 160us/step - loss: 0.5679 - acc: 0.7062 - val_loss: 0.6534 - val_acc: 0.6732\n",
      "Epoch 19/150\n",
      "514/514 [==============================] - 0s 162us/step - loss: 0.5675 - acc: 0.7179 - val_loss: 0.6536 - val_acc: 0.6654\n",
      "Epoch 20/150\n",
      "514/514 [==============================] - 0s 161us/step - loss: 0.5784 - acc: 0.7179 - val_loss: 0.6577 - val_acc: 0.6654\n",
      "Epoch 21/150\n",
      "514/514 [==============================] - 0s 161us/step - loss: 0.5625 - acc: 0.7160 - val_loss: 0.6763 - val_acc: 0.6417\n",
      "Epoch 22/150\n",
      "514/514 [==============================] - 0s 159us/step - loss: 0.5699 - acc: 0.6907 - val_loss: 0.6478 - val_acc: 0.6732\n",
      "Epoch 23/150\n",
      "514/514 [==============================] - 0s 161us/step - loss: 0.5686 - acc: 0.7043 - val_loss: 0.6274 - val_acc: 0.6811\n",
      "Epoch 24/150\n",
      "514/514 [==============================] - 0s 161us/step - loss: 0.5646 - acc: 0.7140 - val_loss: 0.6482 - val_acc: 0.6457\n",
      "Epoch 25/150\n",
      "514/514 [==============================] - 0s 158us/step - loss: 0.5537 - acc: 0.7354 - val_loss: 0.6633 - val_acc: 0.6575\n",
      "Epoch 26/150\n",
      "514/514 [==============================] - 0s 159us/step - loss: 0.5594 - acc: 0.7412 - val_loss: 0.6464 - val_acc: 0.6772\n",
      "Epoch 27/150\n",
      "514/514 [==============================] - 0s 161us/step - loss: 0.5583 - acc: 0.7315 - val_loss: 0.6402 - val_acc: 0.6614\n",
      "Epoch 28/150\n",
      "514/514 [==============================] - 0s 159us/step - loss: 0.5532 - acc: 0.7335 - val_loss: 0.6605 - val_acc: 0.6811\n",
      "Epoch 29/150\n",
      "514/514 [==============================] - 0s 161us/step - loss: 0.5545 - acc: 0.7257 - val_loss: 0.6735 - val_acc: 0.6260\n",
      "Epoch 30/150\n",
      "514/514 [==============================] - 0s 162us/step - loss: 0.5560 - acc: 0.7374 - val_loss: 0.6253 - val_acc: 0.6772\n",
      "Epoch 31/150\n",
      "514/514 [==============================] - 0s 161us/step - loss: 0.5616 - acc: 0.7062 - val_loss: 0.6330 - val_acc: 0.6693\n",
      "Epoch 32/150\n",
      "514/514 [==============================] - 0s 159us/step - loss: 0.5466 - acc: 0.7374 - val_loss: 0.6312 - val_acc: 0.6890\n",
      "Epoch 33/150\n",
      "514/514 [==============================] - 0s 160us/step - loss: 0.5447 - acc: 0.7412 - val_loss: 0.6527 - val_acc: 0.6181\n",
      "Epoch 34/150\n",
      "514/514 [==============================] - 0s 160us/step - loss: 0.5483 - acc: 0.7315 - val_loss: 0.6390 - val_acc: 0.6496\n",
      "Epoch 35/150\n",
      "514/514 [==============================] - 0s 159us/step - loss: 0.5443 - acc: 0.7335 - val_loss: 0.6219 - val_acc: 0.6693\n",
      "Epoch 36/150\n",
      "514/514 [==============================] - 0s 161us/step - loss: 0.5436 - acc: 0.7393 - val_loss: 0.6347 - val_acc: 0.6575\n",
      "Epoch 37/150\n",
      "514/514 [==============================] - 0s 161us/step - loss: 0.5474 - acc: 0.7198 - val_loss: 0.6662 - val_acc: 0.6417\n",
      "Epoch 38/150\n",
      "514/514 [==============================] - 0s 161us/step - loss: 0.5430 - acc: 0.7412 - val_loss: 0.6197 - val_acc: 0.6811\n",
      "Epoch 39/150\n",
      "514/514 [==============================] - 0s 161us/step - loss: 0.5307 - acc: 0.7393 - val_loss: 0.6567 - val_acc: 0.6693\n",
      "Epoch 40/150\n",
      "514/514 [==============================] - 0s 160us/step - loss: 0.5419 - acc: 0.7510 - val_loss: 0.6746 - val_acc: 0.6496\n",
      "Epoch 41/150\n",
      "514/514 [==============================] - 0s 161us/step - loss: 0.5613 - acc: 0.7237 - val_loss: 0.6222 - val_acc: 0.6772\n",
      "Epoch 42/150\n",
      "514/514 [==============================] - 0s 160us/step - loss: 0.5430 - acc: 0.7257 - val_loss: 0.6210 - val_acc: 0.7087\n",
      "Epoch 43/150\n",
      "514/514 [==============================] - 0s 161us/step - loss: 0.5425 - acc: 0.7218 - val_loss: 0.6363 - val_acc: 0.6772\n",
      "Epoch 44/150\n",
      "514/514 [==============================] - 0s 161us/step - loss: 0.5283 - acc: 0.7451 - val_loss: 0.6437 - val_acc: 0.6732\n",
      "Epoch 45/150\n",
      "514/514 [==============================] - 0s 161us/step - loss: 0.5338 - acc: 0.7335 - val_loss: 0.6254 - val_acc: 0.6811\n",
      "Epoch 46/150\n",
      "514/514 [==============================] - 0s 161us/step - loss: 0.5389 - acc: 0.7393 - val_loss: 0.6340 - val_acc: 0.6535\n",
      "Epoch 47/150\n",
      "514/514 [==============================] - 0s 160us/step - loss: 0.5266 - acc: 0.7412 - val_loss: 0.6290 - val_acc: 0.6614\n",
      "Epoch 48/150\n",
      "514/514 [==============================] - 0s 161us/step - loss: 0.5270 - acc: 0.7626 - val_loss: 0.6202 - val_acc: 0.6772\n",
      "Epoch 49/150\n",
      "514/514 [==============================] - 0s 160us/step - loss: 0.5449 - acc: 0.7432 - val_loss: 0.6057 - val_acc: 0.6929\n",
      "Epoch 50/150\n",
      "514/514 [==============================] - 0s 161us/step - loss: 0.5256 - acc: 0.7374 - val_loss: 0.6365 - val_acc: 0.7008\n",
      "Epoch 51/150\n",
      "514/514 [==============================] - 0s 160us/step - loss: 0.5259 - acc: 0.7432 - val_loss: 0.6415 - val_acc: 0.6614\n",
      "Epoch 52/150\n",
      "514/514 [==============================] - 0s 164us/step - loss: 0.5189 - acc: 0.7646 - val_loss: 0.6236 - val_acc: 0.6811\n",
      "Epoch 53/150\n",
      "514/514 [==============================] - 0s 159us/step - loss: 0.5177 - acc: 0.7568 - val_loss: 0.6139 - val_acc: 0.6929\n",
      "Epoch 54/150\n",
      "514/514 [==============================] - 0s 161us/step - loss: 0.5218 - acc: 0.7549 - val_loss: 0.6124 - val_acc: 0.6850\n",
      "Epoch 55/150\n",
      "514/514 [==============================] - 0s 162us/step - loss: 0.5208 - acc: 0.7354 - val_loss: 0.6403 - val_acc: 0.6693\n",
      "Epoch 56/150\n",
      "514/514 [==============================] - 0s 159us/step - loss: 0.5248 - acc: 0.7354 - val_loss: 0.6594 - val_acc: 0.6850\n",
      "Epoch 57/150\n",
      "514/514 [==============================] - 0s 160us/step - loss: 0.5205 - acc: 0.7549 - val_loss: 0.6115 - val_acc: 0.7047\n",
      "Epoch 58/150\n",
      "514/514 [==============================] - 0s 159us/step - loss: 0.5268 - acc: 0.7626 - val_loss: 0.6156 - val_acc: 0.6969\n",
      "Epoch 59/150\n",
      "514/514 [==============================] - 0s 159us/step - loss: 0.5129 - acc: 0.7704 - val_loss: 0.6042 - val_acc: 0.7008\n",
      "Epoch 60/150\n",
      "514/514 [==============================] - 0s 160us/step - loss: 0.5126 - acc: 0.7646 - val_loss: 0.6197 - val_acc: 0.6929\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 61/150\n",
      "514/514 [==============================] - 0s 160us/step - loss: 0.5069 - acc: 0.7646 - val_loss: 0.6139 - val_acc: 0.6811\n",
      "Epoch 62/150\n",
      "514/514 [==============================] - 0s 162us/step - loss: 0.5102 - acc: 0.7568 - val_loss: 0.6080 - val_acc: 0.7087\n",
      "Epoch 63/150\n",
      "514/514 [==============================] - 0s 162us/step - loss: 0.5139 - acc: 0.7510 - val_loss: 0.6193 - val_acc: 0.6969\n",
      "Epoch 64/150\n",
      "514/514 [==============================] - 0s 162us/step - loss: 0.5180 - acc: 0.7412 - val_loss: 0.6320 - val_acc: 0.6654\n",
      "Epoch 65/150\n",
      "514/514 [==============================] - 0s 160us/step - loss: 0.5178 - acc: 0.7471 - val_loss: 0.6220 - val_acc: 0.6929\n",
      "Epoch 66/150\n",
      "514/514 [==============================] - 0s 161us/step - loss: 0.5078 - acc: 0.7626 - val_loss: 0.5993 - val_acc: 0.7165\n",
      "Epoch 67/150\n",
      "514/514 [==============================] - 0s 163us/step - loss: 0.5147 - acc: 0.7568 - val_loss: 0.6120 - val_acc: 0.6890\n",
      "Epoch 68/150\n",
      "514/514 [==============================] - 0s 162us/step - loss: 0.5054 - acc: 0.7646 - val_loss: 0.6088 - val_acc: 0.7008\n",
      "Epoch 69/150\n",
      "514/514 [==============================] - 0s 160us/step - loss: 0.5166 - acc: 0.7510 - val_loss: 0.6267 - val_acc: 0.6890\n",
      "Epoch 70/150\n",
      "514/514 [==============================] - 0s 161us/step - loss: 0.4986 - acc: 0.7607 - val_loss: 0.6130 - val_acc: 0.7126\n",
      "Epoch 71/150\n",
      "514/514 [==============================] - 0s 160us/step - loss: 0.5043 - acc: 0.7588 - val_loss: 0.6020 - val_acc: 0.7047\n",
      "Epoch 72/150\n",
      "514/514 [==============================] - 0s 161us/step - loss: 0.4917 - acc: 0.7724 - val_loss: 0.6481 - val_acc: 0.6378\n",
      "Epoch 73/150\n",
      "514/514 [==============================] - 0s 160us/step - loss: 0.5103 - acc: 0.7763 - val_loss: 0.6526 - val_acc: 0.6417\n",
      "Epoch 74/150\n",
      "514/514 [==============================] - 0s 162us/step - loss: 0.5047 - acc: 0.7685 - val_loss: 0.6121 - val_acc: 0.6969\n",
      "Epoch 75/150\n",
      "514/514 [==============================] - 0s 160us/step - loss: 0.4923 - acc: 0.7646 - val_loss: 0.6154 - val_acc: 0.7008\n",
      "Epoch 76/150\n",
      "514/514 [==============================] - 0s 160us/step - loss: 0.4990 - acc: 0.7549 - val_loss: 0.6145 - val_acc: 0.7047\n",
      "Epoch 77/150\n",
      "514/514 [==============================] - 0s 162us/step - loss: 0.4964 - acc: 0.7724 - val_loss: 0.6088 - val_acc: 0.7165\n",
      "Epoch 78/150\n",
      "514/514 [==============================] - 0s 161us/step - loss: 0.4943 - acc: 0.7685 - val_loss: 0.6149 - val_acc: 0.7087\n",
      "Epoch 79/150\n",
      "514/514 [==============================] - 0s 159us/step - loss: 0.4865 - acc: 0.7743 - val_loss: 0.6030 - val_acc: 0.7087\n",
      "Epoch 80/150\n",
      "514/514 [==============================] - 0s 160us/step - loss: 0.4916 - acc: 0.7588 - val_loss: 0.6034 - val_acc: 0.7047\n",
      "Epoch 81/150\n",
      "514/514 [==============================] - 0s 161us/step - loss: 0.5015 - acc: 0.7549 - val_loss: 0.6089 - val_acc: 0.7165\n",
      "Epoch 82/150\n",
      "514/514 [==============================] - 0s 160us/step - loss: 0.4916 - acc: 0.7568 - val_loss: 0.6348 - val_acc: 0.6732\n",
      "Epoch 83/150\n",
      "514/514 [==============================] - 0s 160us/step - loss: 0.4926 - acc: 0.7665 - val_loss: 0.6393 - val_acc: 0.7008\n",
      "Epoch 84/150\n",
      "514/514 [==============================] - 0s 162us/step - loss: 0.5028 - acc: 0.7704 - val_loss: 0.6064 - val_acc: 0.7008\n",
      "Epoch 85/150\n",
      "514/514 [==============================] - 0s 159us/step - loss: 0.4965 - acc: 0.7665 - val_loss: 0.6035 - val_acc: 0.7165\n",
      "Epoch 86/150\n",
      "514/514 [==============================] - 0s 161us/step - loss: 0.4868 - acc: 0.7782 - val_loss: 0.6328 - val_acc: 0.6929\n",
      "Epoch 87/150\n",
      "514/514 [==============================] - 0s 161us/step - loss: 0.4901 - acc: 0.7704 - val_loss: 0.6024 - val_acc: 0.7165\n",
      "Epoch 88/150\n",
      "514/514 [==============================] - 0s 159us/step - loss: 0.5027 - acc: 0.7490 - val_loss: 0.6162 - val_acc: 0.7008\n",
      "Epoch 89/150\n",
      "514/514 [==============================] - 0s 158us/step - loss: 0.4799 - acc: 0.7685 - val_loss: 0.6486 - val_acc: 0.7008\n",
      "Epoch 90/150\n",
      "514/514 [==============================] - 0s 161us/step - loss: 0.5003 - acc: 0.7743 - val_loss: 0.6229 - val_acc: 0.6929\n",
      "Epoch 91/150\n",
      "514/514 [==============================] - 0s 162us/step - loss: 0.4856 - acc: 0.7685 - val_loss: 0.6395 - val_acc: 0.6850\n",
      "Epoch 92/150\n",
      "514/514 [==============================] - 0s 161us/step - loss: 0.4890 - acc: 0.7763 - val_loss: 0.6498 - val_acc: 0.6850\n",
      "Epoch 93/150\n",
      "514/514 [==============================] - 0s 160us/step - loss: 0.4901 - acc: 0.7588 - val_loss: 0.6122 - val_acc: 0.6929\n",
      "Epoch 94/150\n",
      "514/514 [==============================] - 0s 159us/step - loss: 0.4909 - acc: 0.7607 - val_loss: 0.6250 - val_acc: 0.6929\n",
      "Epoch 95/150\n",
      "514/514 [==============================] - 0s 161us/step - loss: 0.4797 - acc: 0.7743 - val_loss: 0.6131 - val_acc: 0.7087\n",
      "Epoch 96/150\n",
      "514/514 [==============================] - 0s 161us/step - loss: 0.4959 - acc: 0.7665 - val_loss: 0.6089 - val_acc: 0.7087\n",
      "Epoch 97/150\n",
      "514/514 [==============================] - 0s 160us/step - loss: 0.4833 - acc: 0.7529 - val_loss: 0.6201 - val_acc: 0.7047\n",
      "Epoch 98/150\n",
      "514/514 [==============================] - 0s 163us/step - loss: 0.4823 - acc: 0.7704 - val_loss: 0.6492 - val_acc: 0.7087\n",
      "Epoch 99/150\n",
      "514/514 [==============================] - 0s 161us/step - loss: 0.4847 - acc: 0.7802 - val_loss: 0.6287 - val_acc: 0.6969\n",
      "Epoch 100/150\n",
      "514/514 [==============================] - 0s 161us/step - loss: 0.4778 - acc: 0.7704 - val_loss: 0.6203 - val_acc: 0.7126\n",
      "Epoch 101/150\n",
      "514/514 [==============================] - 0s 165us/step - loss: 0.4798 - acc: 0.7665 - val_loss: 0.6229 - val_acc: 0.6890\n",
      "Epoch 102/150\n",
      "514/514 [==============================] - 0s 160us/step - loss: 0.4903 - acc: 0.7549 - val_loss: 0.6156 - val_acc: 0.7047\n",
      "Epoch 103/150\n",
      "514/514 [==============================] - 0s 162us/step - loss: 0.4753 - acc: 0.7918 - val_loss: 0.6457 - val_acc: 0.6929\n",
      "Epoch 104/150\n",
      "514/514 [==============================] - 0s 161us/step - loss: 0.4852 - acc: 0.7802 - val_loss: 0.6161 - val_acc: 0.7126\n",
      "Epoch 105/150\n",
      "514/514 [==============================] - 0s 162us/step - loss: 0.4774 - acc: 0.7665 - val_loss: 0.6241 - val_acc: 0.7047\n",
      "Epoch 106/150\n",
      "514/514 [==============================] - 0s 163us/step - loss: 0.4768 - acc: 0.7802 - val_loss: 0.6140 - val_acc: 0.7047\n",
      "Epoch 107/150\n",
      "514/514 [==============================] - 0s 162us/step - loss: 0.4854 - acc: 0.7724 - val_loss: 0.6268 - val_acc: 0.6969\n",
      "Epoch 108/150\n",
      "514/514 [==============================] - 0s 161us/step - loss: 0.4808 - acc: 0.7743 - val_loss: 0.6162 - val_acc: 0.6969\n",
      "Epoch 109/150\n",
      "514/514 [==============================] - 0s 161us/step - loss: 0.4833 - acc: 0.7899 - val_loss: 0.6322 - val_acc: 0.6850\n",
      "Epoch 110/150\n",
      "514/514 [==============================] - 0s 159us/step - loss: 0.4666 - acc: 0.7899 - val_loss: 0.6305 - val_acc: 0.6969\n",
      "Epoch 111/150\n",
      "514/514 [==============================] - 0s 160us/step - loss: 0.4751 - acc: 0.7821 - val_loss: 0.6148 - val_acc: 0.7087\n",
      "Epoch 112/150\n",
      "514/514 [==============================] - 0s 161us/step - loss: 0.4676 - acc: 0.7879 - val_loss: 0.6140 - val_acc: 0.7047\n",
      "Epoch 113/150\n",
      "514/514 [==============================] - 0s 162us/step - loss: 0.4731 - acc: 0.7860 - val_loss: 0.6253 - val_acc: 0.7126\n",
      "Epoch 114/150\n",
      "514/514 [==============================] - 0s 160us/step - loss: 0.4730 - acc: 0.7763 - val_loss: 0.6333 - val_acc: 0.7047\n",
      "Epoch 115/150\n",
      "514/514 [==============================] - 0s 161us/step - loss: 0.4693 - acc: 0.7782 - val_loss: 0.6176 - val_acc: 0.7205\n",
      "Epoch 116/150\n",
      "514/514 [==============================] - 0s 161us/step - loss: 0.4656 - acc: 0.7879 - val_loss: 0.6295 - val_acc: 0.7008\n",
      "Epoch 117/150\n",
      "514/514 [==============================] - 0s 161us/step - loss: 0.4646 - acc: 0.7879 - val_loss: 0.6195 - val_acc: 0.7126\n",
      "Epoch 118/150\n",
      "514/514 [==============================] - 0s 162us/step - loss: 0.4645 - acc: 0.7840 - val_loss: 0.6214 - val_acc: 0.7087\n",
      "Epoch 119/150\n",
      "514/514 [==============================] - 0s 161us/step - loss: 0.4695 - acc: 0.7802 - val_loss: 0.6204 - val_acc: 0.7047\n",
      "Epoch 120/150\n",
      "514/514 [==============================] - 0s 160us/step - loss: 0.4697 - acc: 0.7899 - val_loss: 0.6249 - val_acc: 0.7165\n",
      "Epoch 121/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "514/514 [==============================] - 0s 161us/step - loss: 0.4855 - acc: 0.7724 - val_loss: 0.6213 - val_acc: 0.7165\n",
      "Epoch 122/150\n",
      "514/514 [==============================] - 0s 161us/step - loss: 0.4678 - acc: 0.7840 - val_loss: 0.6124 - val_acc: 0.7087\n",
      "Epoch 123/150\n",
      "514/514 [==============================] - 0s 160us/step - loss: 0.4791 - acc: 0.7685 - val_loss: 0.6285 - val_acc: 0.7205\n",
      "Epoch 124/150\n",
      "514/514 [==============================] - 0s 161us/step - loss: 0.4642 - acc: 0.7879 - val_loss: 0.6343 - val_acc: 0.7047\n",
      "Epoch 125/150\n",
      "514/514 [==============================] - 0s 160us/step - loss: 0.4636 - acc: 0.7743 - val_loss: 0.6128 - val_acc: 0.6969\n",
      "Epoch 126/150\n",
      "514/514 [==============================] - 0s 159us/step - loss: 0.4634 - acc: 0.7802 - val_loss: 0.6239 - val_acc: 0.6969\n",
      "Epoch 127/150\n",
      "514/514 [==============================] - 0s 159us/step - loss: 0.4741 - acc: 0.7665 - val_loss: 0.6200 - val_acc: 0.7165\n",
      "Epoch 128/150\n",
      "514/514 [==============================] - 0s 158us/step - loss: 0.4624 - acc: 0.7724 - val_loss: 0.6131 - val_acc: 0.7126\n",
      "Epoch 129/150\n",
      "514/514 [==============================] - 0s 163us/step - loss: 0.4856 - acc: 0.7665 - val_loss: 0.6092 - val_acc: 0.7087\n",
      "Epoch 130/150\n",
      "514/514 [==============================] - 0s 161us/step - loss: 0.4628 - acc: 0.7899 - val_loss: 0.6247 - val_acc: 0.7087\n",
      "Epoch 131/150\n",
      "514/514 [==============================] - 0s 160us/step - loss: 0.4585 - acc: 0.7938 - val_loss: 0.6589 - val_acc: 0.6850\n",
      "Epoch 132/150\n",
      "514/514 [==============================] - 0s 161us/step - loss: 0.4748 - acc: 0.7763 - val_loss: 0.6353 - val_acc: 0.6929\n",
      "Epoch 133/150\n",
      "514/514 [==============================] - 0s 159us/step - loss: 0.4690 - acc: 0.7840 - val_loss: 0.6361 - val_acc: 0.7165\n",
      "Epoch 134/150\n",
      "514/514 [==============================] - 0s 160us/step - loss: 0.4613 - acc: 0.7782 - val_loss: 0.6216 - val_acc: 0.6890\n",
      "Epoch 135/150\n",
      "514/514 [==============================] - 0s 161us/step - loss: 0.4626 - acc: 0.7763 - val_loss: 0.6385 - val_acc: 0.7047\n",
      "Epoch 136/150\n",
      "514/514 [==============================] - 0s 160us/step - loss: 0.4743 - acc: 0.7724 - val_loss: 0.6525 - val_acc: 0.7087\n",
      "Epoch 137/150\n",
      "514/514 [==============================] - 0s 161us/step - loss: 0.4600 - acc: 0.7840 - val_loss: 0.6545 - val_acc: 0.7008\n",
      "Epoch 138/150\n",
      "514/514 [==============================] - 0s 161us/step - loss: 0.4679 - acc: 0.7626 - val_loss: 0.6231 - val_acc: 0.7047\n",
      "Epoch 139/150\n",
      "514/514 [==============================] - 0s 163us/step - loss: 0.4657 - acc: 0.7938 - val_loss: 0.6141 - val_acc: 0.7126\n",
      "Epoch 140/150\n",
      "514/514 [==============================] - 0s 160us/step - loss: 0.4649 - acc: 0.7957 - val_loss: 0.6415 - val_acc: 0.7047\n",
      "Epoch 141/150\n",
      "514/514 [==============================] - 0s 163us/step - loss: 0.4683 - acc: 0.7704 - val_loss: 0.6206 - val_acc: 0.7205\n",
      "Epoch 142/150\n",
      "514/514 [==============================] - 0s 161us/step - loss: 0.4750 - acc: 0.7821 - val_loss: 0.6331 - val_acc: 0.7008\n",
      "Epoch 143/150\n",
      "514/514 [==============================] - 0s 160us/step - loss: 0.4577 - acc: 0.7782 - val_loss: 0.6399 - val_acc: 0.7087\n",
      "Epoch 144/150\n",
      "514/514 [==============================] - 0s 159us/step - loss: 0.4556 - acc: 0.7840 - val_loss: 0.6611 - val_acc: 0.6929\n",
      "Epoch 145/150\n",
      "514/514 [==============================] - 0s 160us/step - loss: 0.4692 - acc: 0.7899 - val_loss: 0.6147 - val_acc: 0.7205\n",
      "Epoch 146/150\n",
      "514/514 [==============================] - 0s 161us/step - loss: 0.4585 - acc: 0.7743 - val_loss: 0.6148 - val_acc: 0.7283\n",
      "Epoch 147/150\n",
      "514/514 [==============================] - 0s 161us/step - loss: 0.4714 - acc: 0.7763 - val_loss: 0.6157 - val_acc: 0.7047\n",
      "Epoch 148/150\n",
      "514/514 [==============================] - 0s 159us/step - loss: 0.4611 - acc: 0.7782 - val_loss: 0.6332 - val_acc: 0.7165\n",
      "Epoch 149/150\n",
      "514/514 [==============================] - 0s 161us/step - loss: 0.4633 - acc: 0.7821 - val_loss: 0.6419 - val_acc: 0.6969\n",
      "Epoch 150/150\n",
      "514/514 [==============================] - 0s 161us/step - loss: 0.4607 - acc: 0.7743 - val_loss: 0.6305 - val_acc: 0.7087\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f25c43f9be0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=47)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(12, input_dim=8, activation='relu'))\n",
    "model.add(Dense(8, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=150, batch_size=10, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. K-fold Cross Validation\n",
    "It splits the **training dataset** into **k subsets** and takes turns training models on all subsets except one which is held out, and evaluating model performance on the **held out validation dataset**. The process is repeated until all subsets are given an opportunity to be the **held out validation set**. The performance measure is then **averaged across all models** that are created. \n",
    "\n",
    "Cross-validation is often not used for evaluating deep learning models because of the greater computational expense. For example k-fold cross-validation is often used with 5 or 10 folds. As such, 5 or 10 models must be constructed and evaluated, greatly adding to the evaluation time of a model. Nevertheless, when the problem is small enough or if you have sufficient compute resources, k-fold cross-validation can give you a less biased estimate of the performance of your model. \n",
    "\n",
    "In the example below we use the handy **StratifiedKFold** class from the scikit-learn library to split up the training dataset into 10 folds. The folds are **stratiﬁed**, meaning that the algorithm **balances the number of instances** of each class in each fold. The example creates and evaluates 10 models using the 10 splits of the data and collects all of the scores. The verbose output for each epoch is turned oﬀ by passing verbose=0 to the **`fit()`** and **`evaluate()`** functions on the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 0.68\n",
      "acc: 0.66\n",
      "acc: 0.64\n",
      "acc: 0.73\n",
      "acc: 0.79\n",
      "acc: 0.73\n",
      "acc: 0.73\n",
      "acc: 0.78\n",
      "acc: 0.66\n",
      "acc: 0.63\n",
      "mean: 0.70 std: (+/- 0.05)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "Kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=47)\n",
    "\n",
    "cv_scores = []\n",
    "for tr, te in Kfold.split(X,y):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(12, input_dim=8, activation='relu'))\n",
    "    model.add(Dense(8, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    model.fit(X[tr], y[tr], epochs=150, batch_size=10, verbose=0)\n",
    "    scores = model.evaluate(X[te], y[te], verbose=0)\n",
    "    print(\"%s: %.2f\" %(model.metrics_names[1], scores[1]))\n",
    "    cv_scores.append(scores[1])\n",
    "    \n",
    "print(\"mean: %.2f std: (+/- %.2f)\" %(np.mean(cv_scores), np.std(cv_scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Here, can we streamline it ? \n",
    "\n",
    "SKLearn offers 'model', 'validation tool', 'tuning hyperparameters(Grid_Search)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 1) Evaluate Models with Cross-Validation\n",
    "\n",
    "**KerasClassifier()** and **KerasRegressor()** in keras take an argument **build_fn** which is the **name** of the function to call to create your model. \n",
    "- You should define the **function** that define your model, compile your model, and returns it. Let's deﬁne a function **`create_model()`** that create a simple multilayer neural network for the problem.  \n",
    "\n",
    "- We also pass in additional arguments of **epochs=150** and **batch_size=10**. These are automatically bundled up and passed on to the **`fit()`** function which is called internally by the **KerasClassifier()**. \n",
    "\n",
    "To split dataset: use **`StratifiedKfold()`**\n",
    "\n",
    "To evaluate the performance using the cross-validation scheme: use **`cross_val_score()`**. \n",
    "\n",
    "You can see that when the Keras model is wrapped that estimating model accuracy can be **greatly streamlined**, compared to the manual enumeration of cross-validation folds. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7226247434023709"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# MLP(Multi Layered Perceptrons) for 'Pima_Indians_Dataset' with 10-fold_cross_validation via sklearn..\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "# Define the function to create model, required for 'KerasClassifier()'\n",
    "def create_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(12, input_dim=8, activation='relu'))\n",
    "    model.add(Dense(8, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return(model)\n",
    "\n",
    "# this already contains 'fit()'\n",
    "model = KerasClassifier(build_fn=create_model, epochs=150, batch_size=10, verbose=0)\n",
    "\n",
    "\n",
    "# splitting dataset + fit and evaluation\n",
    "# A total of 10 models are created and evaluated and the ﬁnal average accuracy is displayed\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=47)\n",
    "result = cross_val_score(model, X, y, cv = kfold)\n",
    "\n",
    "result.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 2) Grid-Search CV!! hyper parameters tuning\n",
    "The previous example showed how easy it is to **wrap your deep learning model from Keras** and use it in functions from the scikit-learn library. In this example we go a step further. We already know we can provide arguments to the fit() function. **The function** that we specify to the **build_fn** argument when creating the `KerasClassifier()` wrapper can also take **arguments**. We can use these arguments to further customize the construction of the model. \n",
    "\n",
    "We use **GridSearchCV** to evaluate different conﬁgurations for our neural network model and report on the combination that provides the best estimated performance. \n",
    "\n",
    "The **create_model()** function is deﬁned to take two arguments **optimizer** and **init**, both of which must have default values. This will allow us to evaluate the effect of using different **1)optimization algorithms** and **2)weight initialization schemes** for our network. \n",
    "\n",
    ">After creating our model, we deﬁne **arrays of values** for the parameter we wish to search, speciﬁcally\n",
    ">- **Optimizers** for searching diﬀerent weight values\n",
    ">- **Initializers** for preparing the network weights using dfferent schemes\n",
    ">- Number of **epochs** for training the model for different number of exposures to the training dataset.\n",
    ">- **Batches** for varying the number of samples before weight updates\n",
    "\n",
    "The options are specfied into a **dictionary** and passed to the conﬁguration of the GridSearchCV scikit-learn class.\n",
    "\n",
    "This class will evaluate a version of our neural network model for each combination of parameters (2 × 3 × 3 × 3) for the combinations of **optimizers**, **initializations**, **epochs** and **batches**). Each combination is then evaluated using the default of **3-fold** stratifiedKFold(). \n",
    "\n",
    "Finally, the performance and combination of conﬁgurations for the best model will be displayed, followed by the performance of all combinations of parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best score is 0.748698, using {'batch_size': 10, 'epochs': 100, 'init': 'uniform', 'optimizer': 'adam'}\n",
      "mean: 0.690104 ,std: 0.022628 with {'batch_size': 5, 'epochs': 50, 'init': 'glorot_uniform', 'optimizer': 'rmsprop'}\n",
      "mean: 0.686198 ,std: 0.049445 with {'batch_size': 5, 'epochs': 50, 'init': 'glorot_uniform', 'optimizer': 'adam'}\n",
      "mean: 0.694010 ,std: 0.012075 with {'batch_size': 5, 'epochs': 50, 'init': 'normal', 'optimizer': 'rmsprop'}\n",
      "mean: 0.725260 ,std: 0.018136 with {'batch_size': 5, 'epochs': 50, 'init': 'normal', 'optimizer': 'adam'}\n",
      "mean: 0.684896 ,std: 0.035277 with {'batch_size': 5, 'epochs': 50, 'init': 'uniform', 'optimizer': 'rmsprop'}\n",
      "mean: 0.707031 ,std: 0.005524 with {'batch_size': 5, 'epochs': 50, 'init': 'uniform', 'optimizer': 'adam'}\n",
      "mean: 0.692708 ,std: 0.018414 with {'batch_size': 5, 'epochs': 100, 'init': 'glorot_uniform', 'optimizer': 'rmsprop'}\n",
      "mean: 0.714844 ,std: 0.031412 with {'batch_size': 5, 'epochs': 100, 'init': 'glorot_uniform', 'optimizer': 'adam'}\n",
      "mean: 0.700521 ,std: 0.016053 with {'batch_size': 5, 'epochs': 100, 'init': 'normal', 'optimizer': 'rmsprop'}\n",
      "mean: 0.722656 ,std: 0.029232 with {'batch_size': 5, 'epochs': 100, 'init': 'normal', 'optimizer': 'adam'}\n",
      "mean: 0.712240 ,std: 0.038976 with {'batch_size': 5, 'epochs': 100, 'init': 'uniform', 'optimizer': 'rmsprop'}\n",
      "mean: 0.740885 ,std: 0.025780 with {'batch_size': 5, 'epochs': 100, 'init': 'uniform', 'optimizer': 'adam'}\n",
      "mean: 0.675781 ,std: 0.019401 with {'batch_size': 5, 'epochs': 150, 'init': 'glorot_uniform', 'optimizer': 'rmsprop'}\n",
      "mean: 0.691406 ,std: 0.039964 with {'batch_size': 5, 'epochs': 150, 'init': 'glorot_uniform', 'optimizer': 'adam'}\n",
      "mean: 0.727865 ,std: 0.030145 with {'batch_size': 5, 'epochs': 150, 'init': 'normal', 'optimizer': 'rmsprop'}\n",
      "mean: 0.735677 ,std: 0.039879 with {'batch_size': 5, 'epochs': 150, 'init': 'normal', 'optimizer': 'adam'}\n",
      "mean: 0.735677 ,std: 0.020505 with {'batch_size': 5, 'epochs': 150, 'init': 'uniform', 'optimizer': 'rmsprop'}\n",
      "mean: 0.734375 ,std: 0.006379 with {'batch_size': 5, 'epochs': 150, 'init': 'uniform', 'optimizer': 'adam'}\n",
      "mean: 0.634115 ,std: 0.008027 with {'batch_size': 10, 'epochs': 50, 'init': 'glorot_uniform', 'optimizer': 'rmsprop'}\n",
      "mean: 0.671875 ,std: 0.022097 with {'batch_size': 10, 'epochs': 50, 'init': 'glorot_uniform', 'optimizer': 'adam'}\n",
      "mean: 0.696615 ,std: 0.019225 with {'batch_size': 10, 'epochs': 50, 'init': 'normal', 'optimizer': 'rmsprop'}\n",
      "mean: 0.703125 ,std: 0.019401 with {'batch_size': 10, 'epochs': 50, 'init': 'normal', 'optimizer': 'adam'}\n",
      "mean: 0.717448 ,std: 0.008027 with {'batch_size': 10, 'epochs': 50, 'init': 'uniform', 'optimizer': 'rmsprop'}\n",
      "mean: 0.669271 ,std: 0.028587 with {'batch_size': 10, 'epochs': 50, 'init': 'uniform', 'optimizer': 'adam'}\n",
      "mean: 0.589844 ,std: 0.148882 with {'batch_size': 10, 'epochs': 100, 'init': 'glorot_uniform', 'optimizer': 'rmsprop'}\n",
      "mean: 0.684896 ,std: 0.022628 with {'batch_size': 10, 'epochs': 100, 'init': 'glorot_uniform', 'optimizer': 'adam'}\n",
      "mean: 0.721354 ,std: 0.027498 with {'batch_size': 10, 'epochs': 100, 'init': 'normal', 'optimizer': 'rmsprop'}\n",
      "mean: 0.717448 ,std: 0.018136 with {'batch_size': 10, 'epochs': 100, 'init': 'normal', 'optimizer': 'adam'}\n",
      "mean: 0.742187 ,std: 0.011500 with {'batch_size': 10, 'epochs': 100, 'init': 'uniform', 'optimizer': 'rmsprop'}\n",
      "mean: 0.748698 ,std: 0.037240 with {'batch_size': 10, 'epochs': 100, 'init': 'uniform', 'optimizer': 'adam'}\n",
      "mean: 0.720052 ,std: 0.037783 with {'batch_size': 10, 'epochs': 150, 'init': 'glorot_uniform', 'optimizer': 'rmsprop'}\n",
      "mean: 0.725260 ,std: 0.013279 with {'batch_size': 10, 'epochs': 150, 'init': 'glorot_uniform', 'optimizer': 'adam'}\n",
      "mean: 0.733073 ,std: 0.024774 with {'batch_size': 10, 'epochs': 150, 'init': 'normal', 'optimizer': 'rmsprop'}\n",
      "mean: 0.713542 ,std: 0.021236 with {'batch_size': 10, 'epochs': 150, 'init': 'normal', 'optimizer': 'adam'}\n",
      "mean: 0.718750 ,std: 0.013902 with {'batch_size': 10, 'epochs': 150, 'init': 'uniform', 'optimizer': 'rmsprop'}\n",
      "mean: 0.735677 ,std: 0.025582 with {'batch_size': 10, 'epochs': 150, 'init': 'uniform', 'optimizer': 'adam'}\n",
      "mean: 0.657552 ,std: 0.028587 with {'batch_size': 20, 'epochs': 50, 'init': 'glorot_uniform', 'optimizer': 'rmsprop'}\n",
      "mean: 0.535156 ,std: 0.151892 with {'batch_size': 20, 'epochs': 50, 'init': 'glorot_uniform', 'optimizer': 'adam'}\n",
      "mean: 0.697917 ,std: 0.020752 with {'batch_size': 20, 'epochs': 50, 'init': 'normal', 'optimizer': 'rmsprop'}\n",
      "mean: 0.707031 ,std: 0.005524 with {'batch_size': 20, 'epochs': 50, 'init': 'normal', 'optimizer': 'adam'}\n",
      "mean: 0.661458 ,std: 0.018414 with {'batch_size': 20, 'epochs': 50, 'init': 'uniform', 'optimizer': 'rmsprop'}\n",
      "mean: 0.691406 ,std: 0.005524 with {'batch_size': 20, 'epochs': 50, 'init': 'uniform', 'optimizer': 'adam'}\n",
      "mean: 0.686198 ,std: 0.057439 with {'batch_size': 20, 'epochs': 100, 'init': 'glorot_uniform', 'optimizer': 'rmsprop'}\n",
      "mean: 0.696615 ,std: 0.034401 with {'batch_size': 20, 'epochs': 100, 'init': 'glorot_uniform', 'optimizer': 'adam'}\n",
      "mean: 0.720052 ,std: 0.007366 with {'batch_size': 20, 'epochs': 100, 'init': 'normal', 'optimizer': 'rmsprop'}\n",
      "mean: 0.718750 ,std: 0.022097 with {'batch_size': 20, 'epochs': 100, 'init': 'normal', 'optimizer': 'adam'}\n",
      "mean: 0.707031 ,std: 0.025315 with {'batch_size': 20, 'epochs': 100, 'init': 'uniform', 'optimizer': 'rmsprop'}\n",
      "mean: 0.686198 ,std: 0.017566 with {'batch_size': 20, 'epochs': 100, 'init': 'uniform', 'optimizer': 'adam'}\n",
      "mean: 0.695312 ,std: 0.011049 with {'batch_size': 20, 'epochs': 150, 'init': 'glorot_uniform', 'optimizer': 'rmsprop'}\n",
      "mean: 0.566406 ,std: 0.162755 with {'batch_size': 20, 'epochs': 150, 'init': 'glorot_uniform', 'optimizer': 'adam'}\n",
      "mean: 0.730469 ,std: 0.019401 with {'batch_size': 20, 'epochs': 150, 'init': 'normal', 'optimizer': 'rmsprop'}\n",
      "mean: 0.722656 ,std: 0.028705 with {'batch_size': 20, 'epochs': 150, 'init': 'normal', 'optimizer': 'adam'}\n",
      "mean: 0.731771 ,std: 0.006639 with {'batch_size': 20, 'epochs': 150, 'init': 'uniform', 'optimizer': 'rmsprop'}\n",
      "mean: 0.739583 ,std: 0.039365 with {'batch_size': 20, 'epochs': 150, 'init': 'uniform', 'optimizer': 'adam'}\n"
     ]
    }
   ],
   "source": [
    "# MLP(multi layered perceptrons) for Pima Indians Dataset with grid search via sklearn \n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Function to create model, required for KerasClassifier \n",
    "def create_model(optimizer='rmsprop', init='glorot_uniform'): \n",
    "    model = Sequential() \n",
    "    model.add(Dense(12, input_dim=8, kernel_initializer=init, activation='relu')) \n",
    "    model.add(Dense(8, kernel_initializer=init, activation='relu')) \n",
    "    model.add(Dense(1, kernel_initializer=init, activation='sigmoid')) \n",
    "    # Compile model \n",
    "    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy']) \n",
    "    return(model)\n",
    "\n",
    "# create model \n",
    "model = KerasClassifier(build_fn=create_model, verbose=0) \n",
    "\n",
    "# grid search epochs, batch_sizes and optimizers \n",
    "optimizers = ['rmsprop','adam']\n",
    "inits = ['glorot_uniform','normal','uniform']\n",
    "epochs = [50,100,150]\n",
    "batches = [5, 10, 20]\n",
    "\n",
    "param_grid = dict(optimizer=optimizers, init=inits, epochs=epochs, batch_size=batches)\n",
    "grid_result = GridSearchCV(estimator=model, param_grid = param_grid).fit(X,y)\n",
    "\n",
    "# summarize results\n",
    "print('best score is %f, using %s' %(grid_result.best_score_, grid_result.best_params_))\n",
    "\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for i,j,k in zip(means, stds, params):\n",
    "    print('mean: %f ,std: %f with %r' %(i,j,k))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We can see that the grid search discovered that using a uniform initialization scheme, rmsprop optimizer, 150 epochs and a batch size of 5 achieved the best cross-validation score of approximately 75% on this problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
