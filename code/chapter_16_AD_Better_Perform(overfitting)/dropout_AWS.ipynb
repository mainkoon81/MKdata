{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Improve the performance I.\n",
    "### Reduce Overﬁtting With Dropout Regularization\n",
    "- dropout on your input layers\n",
    "- dropout on your hidden layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### What is the dropout Regularization?\n",
    "Dropout is a regularization technique for neural network model. Dropout is a technique where randomly selected neurons are ignored during training. This means that their contribution to the activation of downstream neurons is temporally removed on the forward pass and any weight updates are not applied to the neuron on the backward pass. As a neural network learns, neuron weights settle into their context within the network. Weights of neurons are tuned for speciﬁc features providing some specialization. Neighboring neurons come to rely on this specialization, which if taken too far can result in a fragile model too specialized to the training data (This reliance on context for a neuron during training is referred to as complex co-adaptations). You can imagine that if neurons are randomly dropped out of the network during training, that other neurons will have to step in and handle the representation required to make predictions for the missing neurons. This is believed to result in multiple independent internal representations being learned by the network. \n",
    "\n",
    "The eﬀect is that the network becomes less sensitive to the speciﬁc weights of neurons. This in turn results in a network that is capable of better generalization and is less likely to overﬁt the training data.\n",
    "\n",
    "Dropout is easily implemented by randomly selecting nodes to be dropped-out with a given probability (e.g. 20%) each weight update cycle. This is how Dropout is implemented in Keras. Dropout is only used during the training of a model and is not used when evaluating the skill of the model. \n",
    "\n",
    "In this example, we will evaluate the developed models using scikit-learn with **10-fold cross-validation**, in order to better tease out diﬀerences in the results. There are **60 input** values and a **single output** value and the **input values are standardized** before being used in the network. \n",
    "\n",
    "The baseline neural network model has two hidden layers, the ﬁrst with 60 units and the second with 30. Stochastic gradient descent is used to train the model with a relatively low learning rate and momentum. The full baseline model is listed below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "from keras.optimizers import SGD # Stochastic Gradient Descent\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "np.random.seed(47)\n",
    "df = pd.read_csv('sonar.csv', header=None)\n",
    "data = df.values\n",
    "\n",
    "X = data[:,0:60].astype(float) \n",
    "y = data[:,60]\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(y)\n",
    "\n",
    "encoded_y = encoder.transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline_mean: 0.86 (+/-0.09)\n"
     ]
    }
   ],
   "source": [
    "# the baseline model without drop-out \n",
    "#  Generate an estimated classiﬁcation accuracy....\n",
    "\n",
    "def create_baseline():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(60, input_dim=60, activation='relu', kernel_initializer='normal'))\n",
    "    model.add(Dense(30, activation='relu', kernel_initializer='normal'))\n",
    "    model.add(Dense(1, activation='sigmoid', kernel_initializer='normal'))\n",
    "    \n",
    "    sgd = SGD(lr=0.01, momentum=0.8, decay=0.0, nesterov=False)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "    return(model)\n",
    "\n",
    "estimators =[]\n",
    "estimators.append(('standardize', StandardScaler()))\n",
    "estimators.append(('mlp', KerasClassifier(build_fn=create_baseline, epochs=300, batch_size=16, verbose=0)))\n",
    "\n",
    "pipeline = Pipeline(estimators)\n",
    "\n",
    "kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=47)\n",
    "\n",
    "results = cross_val_score(pipeline, X, encoded_y, cv=kfold)\n",
    "\n",
    "print('Baseline_mean: %.2f (+/-%.2f)' %(results.mean(), results.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Using Dropout on the Visible Layer\n",
    "Dropout can be applied to **input neurons** called the visible layer. In the example below we add a **new Dropout layer** between the input (or visible layer) and the ﬁrst hidden layer. The dropout rate is set to **20%**, meaning one in ﬁve inputs will be randomly excluded from each update cycle. Additionally, as recommended in the original paper on dropout, a constraint is imposed on the weights for each hidden layer, ensuring that the **maximum norm of the weights does not exceed a value of 3**. This is done by setting the **kernel_constraint** argument on the **Dense** class when constructing the layers. \n",
    "\n",
    "The **learning rate** was lifted by one order of magnitude and the **momentum** was increased to 0.9. These increases in the learning rate were also recommended in the original dropout paper. Continuing on from the baseline example above, the code below exercises the same network with input dropout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Add_dropout_inputLayer_mean: 0.87 (+/-0.06)\n"
     ]
    }
   ],
   "source": [
    "# Example of Dropout on the Sonar Dataset: Visible input Layer\n",
    "from keras.layers import Dropout\n",
    "\n",
    "def create_model():  \n",
    "    model = Sequential() \n",
    "    \n",
    "    model.add(Dropout(0.2, input_shape=(60,))) \n",
    "    \n",
    "    model.add(Dense(60, kernel_initializer='normal', activation='relu', kernel_constraint=maxnorm(3)))\n",
    "    model.add(Dense(30, kernel_initializer='normal', activation='relu', kernel_constraint=maxnorm(3)))\n",
    "    model.add(Dense(1, kernel_initializer='normal', activation='sigmoid')) \n",
    "    # Compile model \n",
    "    sgd = SGD(lr=0.1, momentum=0.9, decay=0.0, nesterov=False) \n",
    "    model.compile(loss='binary_crossentropy', optimizer=sgd, metrics=['accuracy']) \n",
    "    return(model)\n",
    "\n",
    "estimators =[]\n",
    "estimators.append(('standardize', StandardScaler()))\n",
    "estimators.append(('mlp', KerasClassifier(build_fn=create_baseline, epochs=300, batch_size=16, verbose=0)))\n",
    "\n",
    "pipeline = Pipeline(estimators)\n",
    "\n",
    "kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=47)\n",
    "\n",
    "results = cross_val_score(pipeline, X, encoded_y, cv=kfold)\n",
    "\n",
    "print('Add_dropout_inputLayer_mean: %.2f (+/-%.2f)' %(results.mean(), results.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Using Dropout on Hidden Layers\n",
    "Dropout can be applied to hidden neurons in the body of your network model. In the example below dropout is applied between the two hidden layers and between the last hidden layer and the output layer. Again a dropout rate of **20%** is used as is a **weight constraint** on those layers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Add_dropout_HiddenLayer_mean: 0.87 (+/-0.09)\n"
     ]
    }
   ],
   "source": [
    "# Example of Dropout with weight constraint on the Sonar Dataset: Hidden Layer\n",
    "from keras.layers import Dropout\n",
    "from keras.constraints import maxnorm\n",
    "\n",
    "def create_model():\n",
    "    model = Sequential() \n",
    "    model.add(Dense(60, input_dim=60, kernel_initializer='normal', activation='relu', kernel_constraint=maxnorm(3))) \n",
    "    \n",
    "    model.add(Dropout(0.2)) \n",
    "    model.add(Dense(30, kernel_initializer='normal', activation='relu', kernel_constraint=maxnorm(3)))\n",
    "    model.add(Dropout(0.2)) \n",
    "    model.add(Dense(1, kernel_initializer='normal', activation='sigmoid'))\n",
    "    \n",
    "    # Compile model \n",
    "    sgd = SGD(lr=0.1, momentum=0.9, decay=0.0, nesterov=False) \n",
    "    model.compile(loss='binary_crossentropy', optimizer=sgd, metrics=['accuracy']) \n",
    "    return(model)\n",
    "\n",
    "estimators =[]\n",
    "estimators.append(('standardize', StandardScaler()))\n",
    "estimators.append(('mlp', KerasClassifier(build_fn=create_baseline, epochs=300, batch_size=16, verbose=0)))\n",
    "\n",
    "pipeline = Pipeline(estimators)\n",
    "\n",
    "kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=47)\n",
    "\n",
    "results = cross_val_score(pipeline, X, encoded_y, cv=kfold)\n",
    "\n",
    "print('Add_dropout_HiddenLayer_mean: %.2f (+/-%.2f)' %(results.mean(), results.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "What if the performance with tuning was worse than the baseline? It is possible that additional training **epochs** are required or that further tuning is required to the **learning rate**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Useful heuristics to consider when using dropout in practice:\n",
    "Generally use a small dropout value of 20%-50% of neurons with 20% providing a good starting point. A probability too low has minimal eﬀect and a value too high results in under-learning by the network.\n",
    "- Use a larger network. You are likely to get better performance when dropout is used on a larger network, giving the model more of an opportunity to learn independent representations.\n",
    "- Use dropout on input (visible) as well as hidden layers. Application of dropout at each layer of the network has shown good results.\n",
    "- Use a large learning rate with decay and a large momentum. Increase your learning rate by a factor of 10 to 100 and use a high momentum value of 0.9 or 0.99.\n",
    "- Constrain the size of network weights. A large learning rate can result in very large network weights. Imposing a constraint on the size of network weights such as max-norm regularization with a size of 4 or 5 has been shown to improve results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
